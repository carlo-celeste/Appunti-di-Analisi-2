\documentclass[11pt,a4paper]{report}

% ============================================================================
% PACCHETTI
% ============================================================================
\usepackage[utf8]{inputenc}
\usepackage[italian]{babel}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{geometry}
\usepackage{fancyhdr}
\usepackage{xcolor}
\usepackage{array}
\usepackage{colortbl}
\usepackage{float}
\usepackage{mdframed}
\usepackage{enumitem}
\usepackage{cancel}
\usepackage[hidelinks]{hyperref}
\usepackage{titlesec}
\usepackage{tikz}
\usepackage{tcolorbox}
\usepackage{graphicx}
\tcbuselibrary{skins,breakable}
\usetikzlibrary{patterns,shadings}

% ============================================================================
% IMPOSTAZIONI PAGINA
% ============================================================================
\geometry{
    top=2cm,
    bottom=2cm,
    left=2.5cm,
    right=2.5cm,
    headheight=1cm
}

% Header personalizzato
\pagestyle{fancy}
\fancyhf{}
\lhead{Carlo Celeste}
\chead{Analisi II: Funzioni di più variabili}
\rhead{\thepage}
\renewcommand{\headrulewidth}{0.5pt}

\setlength{\parindent}{0pt}

% ============================================================================
% COLORI PERSONALIZZATI
% ============================================================================
\definecolor{defcolor}{RGB}{0,102,204}      % Blu per definizioni
\definecolor{thmcolor}{RGB}{204,0,102}      % Magenta per teoremi
\definecolor{notecolor}{RGB}{255,235,205}   % Arancione chiaro per note
\definecolor{tableheader}{RGB}{204,0,102}   % Magenta per header tabelle

% ============================================================================
% FORMATTAZIONE CAPITOLI
% ============================================================================
\titleformat{\chapter}[hang]
    {\normalfont\huge\bfseries}
    {}
    {0pt}
    {\Huge}

% ============================================================================
% LOCALIZZAZIONE ITALIANA
% ============================================================================
\renewcommand{\contentsname}{Indice}

% ============================================================================
% AMBIENTI TEOREMATICI
% ============================================================================
% Stile per definizioni
\newtheoremstyle{defstyle}
    {10pt}                          % Spazio sopra
    {10pt}                          % Spazio sotto
    {\itshape}                      % Font del corpo
    {}                              % Indentazione
    {\bfseries\color{defcolor}}     % Font intestazione
    {.}                             % Punteggiatura dopo intestazione
    { }                             % Spazio dopo intestazione
    {}                              % Specifica intestazione

\theoremstyle{defstyle}
\newtheorem{definizione}{Definizione}[section]

% Stile per teoremi
\newtheoremstyle{thmstyle}
    {10pt}
    {10pt}
    {\itshape}
    {}
    {\bfseries\color{thmcolor}}
    {.}
    { }
    {}

\theoremstyle{thmstyle}
\newtheorem{teorema}{Teorema}[section]
\newtheorem{proposizione}{Proposizione}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{corollario}{Corollario}[section]

% Stile per osservazioni
\theoremstyle{remark}
\newtheorem*{osservazione}{Osservazione}
\newtheorem*{esempio}{Esempio}

% ============================================================================
% AMBIENTI PERSONALIZZATI PER STUDIO
% ============================================================================

% Spazio per note personali
\newcommand{\spazioscritti}[1][4cm]{%
    \vspace{0.5cm}
    \begin{mdframed}[
        backgroundcolor=notecolor,
        linecolor=orange,
        linewidth=1.5pt,
        roundcorner=5pt,
        innertopmargin=10pt,
        innerbottommargin=10pt,
        innerleftmargin=10pt,
        innerrightmargin=10pt
    ]
    {\large\textbf{Spazio per note:}}
    \vspace{#1}
    \end{mdframed}
    \vspace{0.5cm}
}

% ============================================================================
% COMANDI UTILI
% ============================================================================

% Separatore visivo tra sezioni
\newcommand{\separatore}{%
    \vspace{0.8cm}
    \rule{\textwidth}{1pt}
    \vspace{0.8cm}
}

% Mathbf ridefinito
\renewcommand{\mathbf}[1]{\underline{\boldsymbol{#1}}}

% ============================================================================
% NOTAZIONE DEL GRADIENTE
% ============================================================================
\let\oldnabla\nabla
\renewcommand{\nabla}{\oldnabla\mkern-3mu}

% ============================================================================
% INIZIO DOCUMENTO
% ============================================================================
\begin{document}

% Pagina del titolo
\begin{center}
    \vspace*{2cm}
    {\Huge\bfseries Corso di Analisi 2\\[0.3cm] Domande e risposte al macro-argomento:\\[0.3cm] CALCOLO DIFFERENZIALE}\\[1cm]
    {\large Ultima modifica: \today}\\[0.5cm]
    \vspace{2cm}
\end{center}

\newpage

% Indice
\tableofcontents

\newpage

% ============================================================================
% CONTENUTO DEL DOCUMENTO
% ============================================================================



\chapter{Calcolo differenziale}
\section{Funzione a più variabili e campo di esistenza}
\textbf{59. Cosa si intende per {\color{red} una funzione a più variabili}?}

Una funzione a due variabili è una legge che associa ad ogni coppia ordinata di numeri reali $(x,y)$ appartenente ad un sottoinsieme $D \subseteq \mathbb{R}^2$ uno ed un solo numero reale $f(x,y)$ che appertiene al codominio.
\begin{center}
    $f: D \subseteq \mathbb{R}^2 \rightarrow \mathbb{R}$
\end{center}
Ovvero
\begin{center}
    $(x,y) \rightarrow f(x,y)$
\end{center}
Dove $D$ è l'insieme di definizione, $\mathbb{R}$ è il codominio, $(x,y)$ sono le due variabili indipendenti mentre $f(x,y) = z$ è la variabile dipendente.\\

\textbf{60. Cos'è {\color{red} il dominio di una funzione a più variabili}?}
\begin{definizione}[Dominio]
Il dominio o campo di esistenza è l'insieme di tutti i punti dello spazio per la quale la funzione è matematicamente sensata e restituisce un valore reale.
\end{definizione}
Trovare il campo di esistenza di una funzione significa trovare l’insieme massimale rispetto all’inclusione su $\mathbb{R}^2$.\\

\textbf{61. Cosa si intende {\color{red}l'immagine di una funzione a più variabili}?}

L'immagine di una funzione  $f: D \subseteq \mathbb{R}^2 \rightarrow \mathbb{R}$ è l'insieme di tutti i valori che la funzione assume, o formalmente
\begin{center}
    $Im(f)=\{z \in \mathbb{R}: \exists (x,y) \in D \text{ tale che } z =(x,y)\}$
\end{center}

\subsection{Grafico di funzione e sezioni (trasversali e orizzontali)}
\textbf{62. Cosa si intende per {\color{red}grafico di una funzione a più variabili}?}

Il grafico di una funzione $f: D \subseteq \mathbb{R}^2 \rightarrow \mathbb{R}$ è 
\begin{center}
    $\text{graf}f = \{(x,y,z \in \mathbb{R}^3: (x,y) \in D, z=f(x,y) \in \mathbb{R} \} $
\end{center}
\begin{definizione}[Grafico di una funzione a più variabili] E' un sottoinsieme dello spazio $\mathbb{R}^3$, generalmente una superficie nello spazio.
\end{definizione}
Geometricamente, $\forall (x,y) \in D$ consideriamo un punto $(x,y,f(x,y))$ nello spazio, dove $f(x,y)$ è il valore della funzione.\\

\textbf{63. Come si definisce un insieme del piano {\color{red}"scritto" come un'equazione o una disequazione}?}

Sia $f:I \subseteq\mathbb{R}^2 \rightarrow \mathbb{R}$ continua su $I$ e sia il suo grafico $\{(x,y)\in \mathbb{R}^2: y=f(x)\}\subseteq\mathbb{R}^2$.

Il grafico di $f$ descrive non altro che una curva continua sul piano, definita come un arco di curva continua, e in base alla relazione tra $y$ e $f(x)$ potremmo analizzare una sezione del grafico.

\[
\begin{cases}
    y \leq f(x) \text{ si definisce il sottografico di f} \\
    y \geq f(x) \text{ si definisce il sopragrafico di f} \\
\end{cases}
\]
\begin{figure}[H] 
    \centering
    \includegraphics[width=0.5\linewidth]{../Immagini/RelazioneYfunzione}
\end{figure}

\textbf{64. Cosa sono {\color{red}le sezioni di un grafico a piani e le sezioni trasversali}?}

Per visualizzare o rappresentare il grafico di una funzione $f: D \subseteq \mathbb{R}^2 \rightarrow \mathbb{R}$ "seziono" il grafico formando le \textbf{tracce} che intersecano il grafico $ \text{graf}f \subset \mathbb{R}^3$ con piani nello spazio tridimensionale.

Affettando in $\mathbb{R}^3$ il grafico diminuisco la dimensione del mio problema da $\mathbb{R}^3$ a $\mathbb{R}^2$ tracciando delle sezioni del grafico.
\begin{definizione}[Sezione di un grafico]Considerato il grafico di $f$ definita in $\mathbb{R}^2$ con le terne $(x,y,z)$, le sezioni piani con $x=k$ o $y=k$ o $z=k$ dove $k \in \mathbb{R}$.
\end{definizione}
Le due tipologie più importanti di sezioni sono:
\begin{itemize}
    \item \textbf{Sezioni trasversali} ottenute con piani paralleli ai piani coordinati ($xz$ o $yz$);
    \item \textbf{Sezioni orizzontali} ottentute con piani paralleli al piano $xy$ (con piano di equazione $z=k$).
\end{itemize}
Le intersezioni con il grafico generalmente producono una curva nello spazio.\\

\textbf{65. Cosa sono gli insiemi di livello (o {\color{red}curve di livello}) di una funzione di due variabili?}
\begin{definizione}[Insiemi o curve di livello] Gli insiemi di livello, o anche curve di livello, di una funzione $f: D \subseteq \mathbb{R}^2 \rightarrow \mathbb{R}$ sono sottoinsiemi del dominio $D$ su cui la funzione $z =f(x,y)$ assume un valore costante $z=k$.
\end{definizione}
Formalmente,
\begin{center}
    $C_k = \{(x,y) \in D: f(x,y)=k\}$
\end{center}
Le curve di livello non si intersecano mai (eccetto in punti singolari), perché la funzione non può assumere due valori diversi nello stesso punto.\\
\subsection{Curve}
\textbf{66. Cos'è una {\color{red}curva}?}

\begin{definizione}[Curva] Una curva in $\mathbb{R}^n$ è un'applicazione continua definita su un intervallo $I=[a,b] \subseteq \mathbb{R}$
\[
\gamma: I \subseteq \mathbb{R} \rightarrow \mathbb{R}^n
\]
\end{definizione}
Essa è composta da $\gamma_i$ componenti definite su $I \subseteq \mathbb{R}$ e continue con $i=1...n$\\

Il sostegno o immagine di una curva è l'insieme dei punti geometrici toccati dalla curva nello spazio $\mathbb{R}^n$
\[
\text{Sostegno = } \{x\in \mathbb{R}^n: \exists t \in I, x = \gamma(t)\}
\]

\textbf{67. Cosa sono le curve {\color{red} piane, cartesiane o chiuse}?}
\begin{definizione}[Curva Regolare] Sia una curva $\gamma: I \subseteq \mathbb{R} \rightarrow \mathbb{R}^n$ si dice regolare se $\gamma \in C^1(I)$, ossia le sue componenenti sono derivabili e continue su $I$ e la derivata $\gamma'(t) \not = 0$.
\end{definizione}
Una curva regolare non presenta spigoli o cuspidi e in ogni punto è ben definita la retta tangente; infatti con $\gamma'(t) = 0$ la particella si ferma e riparte probabilmente da un'altra direzione creando spigoli.

\begin{definizione}[Curva Cartesiana] E' una curva che nasce direttamente dal grafico della funzione scalare $z=f(x)$. Ogni funzione $f: [a,b] \rightarrow \mathbb{R}$ può essere vista come una curva parametrizzando $x$ con $t$.
\[
\gamma (t) = (t, f(t))
\]
con $t \in [a,b]$
\end{definizione}
Le curve cartesiane sono sempre regolari.

\begin{definizione}[Curva Chiusa] Sia una curva $\gamma: I \subseteq \mathbb{R} \rightarrow \mathbb{R}^n$, si dice chiusa se il punto iniziale coincide con quello finale $\gamma(a) = \gamma(b)$.
\end{definizione}



\textbf{68. Come si può riformulare per le {\color{red}funzioni multivariate il teorema degli zeri}?}

Sia $f:A \subseteq\mathbb{R}^2 \rightarrow \mathbb{R}$ continua su $A$ e supponiamo che esistano due punti $P_1=(x_1,y_1)$ e $P_2=(x_2,y_2)$ che appartengono ad un insieme $A_i \subseteq A$ tale che $f(P_1) \cdot f(P_2) < 0$ con $f(P_1)$ e $f(P_2)$ hanno segno opposto, allora esiste \textbf{almeno} un punto $P_0=(x_0,y_0) \in A_i$ tale che $f(P_0)=0$.\\

Informalmente se il grafico di $f$ passa da valori positivi a negativi (o viceversa) deve per forza "attraversare uno zero", ossia intersecare il piano $z = 0$.






\section{Limiti e continuità}
\textbf{69. Come si definisce una {\color{red}il limite di una funzione a più variabili} attraverso gli intorni?}
\begin{center}
 $f: A \subseteq \mathbb{R}^n \to \mathbb{R}$ tale che $\mathbf{x} \to f(\mathbf{x})$
\end{center}
\begin{definizione}[Limite in $\mathbb{R}^n$] Sia $\mathbf{x}_0 = (x_0^1,...,x_0^n) \in \mathbb{R}^n$ punto di accumulazione per $A$ si dice che $f(\mathbf{x}) \to l$, ovvero che tende a $l$, se $\mathbf{x} \to \mathbf{x}_0$, con $l \in \mathbb{R}^* = \mathbb{R} \cup \{\pm \infty \}$ 
\begin{center}
    $\lim_{\mathbf{x} \to \mathbf{x}_0} f(\mathbf{x}) = l$
\end{center}
\end{definizione}
Visto che $l \in \mathbb{R}^*  $ si parla di intorni di infinito, ossia semirette del tipo $(-\infty, n)$ e $(n, +\infty)$. Quindi, $\forall U \subset \mathbb{R}^* $ di $l$, esiste un intorno sferico di $\mathbf{x}_0$: 
\begin{center}
    $\exists r>0, B(\mathbf{x}_0. r)\subset \mathbb{R}^n$ tale che $f(\mathbf{x}) \in U$ $\forall\mathbf{x}\in B(\mathbf{x}_0, r) \cap (A-\{\mathbf{x}_0\})$ 
\end{center}

\begin{definizione}[$\epsilon-\delta$ con $l$ finito] Se  $\lim_{\mathbf{x} \to \mathbf{x}_0} f(\mathbf{x}) = l$ ed $l$ è finito, allora \begin{center}
    $\forall \epsilon > 0, \exists \delta> 0$ tale che $|f(\mathbf{x}-l)| <\epsilon$ \\ $\forall\mathbf{x}\in(A-\{\mathbf{x}_0\}) $ con $ ||\mathbf{x}-\mathbf{x}_0||<\delta = 0<||\mathbf{x}-\mathbf{x}_0||<\delta$ e $\mathbf{x} \in B(\mathbf{x}_0, \delta)$ 
\end{center}
\end{definizione}

$|f(\mathbf{x}-l)|$ equivale a $l-\epsilon<f(\mathbf{x})<l+\epsilon = B(l, \epsilon)$ con $f(\mathbf{x}) \in B(l, \epsilon)$   


\vspace{0.5em}
Posso rendere $f(\mathbf{x})$ vicino a $l$ quando voglio semplicemente prendendo $\mathbf{x}$ abbastanza vicino a $\mathbf{x}_0$. In parole povere, più ti avvicini a $\mathbf{x}_0$ più il valore della funzione si avvicina a $l$.

\begin{definizione}[$\epsilon-\delta$ con $l = \pm \infty$] Se  $\lim_{\mathbf{x} \to \mathbf{x}_0} f(\mathbf{x}) =  \pm \infty$, quindi il limite diverge, allora 
\begin{center}
    $\forall M >0, \exists\delta>0$ tale che $f(\mathbf{x}) > M$\\
    $\forall\mathbf{x}\in(A-\{\mathbf{x}_0\}) $ con $ ||\mathbf{x}-\mathbf{x}_0||<\delta = 0<||\mathbf{x}-\mathbf{x}_0||<\delta$ e $\mathbf{x} \in B(\mathbf{x}_0, \delta)$
\end{center}
\end{definizione}
\vspace{0.5em}
Posso rendere $f(\mathbf{x})$ grande quanto voglio semplicemente prendendo $\mathbf{x}$ abbastanza vicino a $\mathbf{x}_0$. In poche parole, più ti avvicini a $\mathbf{x}_0$ più il valore della funzione va verso $\infty$.  
\vspace{0.5em}

\textbf{70. Quali sono {\color{red}le proprietà fondamentali del limite in più variabili}?}

Le proprietà fondamentali del limite:
\begin{enumerate}
    \item \textbf{Unicità}
    
    Se esiste, è unico.
    Informalmente, se due limiti della stessa funzione multivariata esistono finiti e danno lo stesso risultato vuol dire che il limite è necesariamente unico.
    Se  $\lim_{\mathbf{x}_1 \to \mathbf{x}_0} f(\mathbf{x}_1) = l_1$ e $\lim_{\mathbf{x}_2 \to \mathbf{x}_0} f(\mathbf{x}_2) = l_2$ esistono finiti e $l_1=l_2$ allora il limite esiste ed è unico.
    
    \item \textbf{Composizione}
    
    Il limite di somma o prodotti di funzioni sono uguali alla somma e prodotti dei limiti; vale anche per il quoziente, il limite del rapporto è il rapporto dei limiti, con la condizione che siano ben definiti i limiti (con il denominatore diverso da 0)
\end{enumerate}

\textbf{71. Quando è {\color{red}un limite ben definito}?}

Per esistere un limite deve essere indipendente dalla direzione di avvicinamento di $(x,y)$ al punto di accumulazione $\longrightarrow$ un limite non deve dipendere dal cammino o dalla direzione scelti con il quale $(x,y)$ si avvvicina il punto di accumulazione.
\\Quindi, se scelti due cammini e i limiti risultano diversi allora il limite per $f(x,y)$ non esiste.\\

\textbf{71. Cosa si intende per {\color{red}limiti ristretti a sottoinsiemi}?}

Se $\lim_{(x,y) \to (x_0,y_0)} f(x,y) = l$ in $A$, allora per ogni sottoinsieme $C \subseteq A$ si ha:
$$\lim_{\substack{(x,y) \to (x_0,y_0) \\ (x,y) \in C}} f(x,y) = l$$
In altre parole, se il limite esiste nell'insieme $A$, allora esiste ed è uguale a $l$ anche quando ci si restringe a qualsiasi sottoinsieme $C$ di $A$.

Ovviamente, il punto $(x_0, y_0)$ deve essere un \textbf{punto di accumulazione} per il dominio $A$.

Formalmente: $(x_0, y_0) \in \overline{A} \setminus \{(x_0, y_0)\}$ oppure in ogni intorno di $(x_0, y_0)$ esistono infiniti punti di $A$ distinti da $(x_0, y_0)$.

Il punto $(x_0, y_0)$ deve essere un punto di accumulazione anche per il sottoinsieme $C$.

Formalmente: $(x_0, y_0) \in \overline{C} \setminus \{(x_0, y_0)\}$.\\

\textbf{72. Come si dimostra la {\color{red}non esistenza di un limite}?}

Se esistono due sottoinsiemi $C_1, C_2 \subseteq A$ tali che $(x_0, y_0)$ è punto di accumulazione per entrambi, e:
$$\lim_{\substack{(x,y) \to (x_0,y_0) \\ (x,y) \in C_1}} f(x,y) = l_1 \neq l_2 = \lim_{\substack{(x,y) \to (x_0,y_0) \\ (x,y) \in C_2}} f(x,y)$$
allora il limite $\lim_{(x,y) \to (x_0,y_0)} f(x,y)$ non esiste.

Se il limite lungo due curve diverse che passano per $(x_0, y_0)$ dà risultati
diversi, allora il limite della funzione per $(x,y) \to (x_0, y_0)$ non esiste.
Questo è una diretta conseguenza del teorema di unicità del limite.

E' possibile osservare che come condizione necessaria per l'esistenza del limite è spesso utilizzato il fatto che i limiti, lungo le rette parallele agli assi coordinati di equazione $y,x = k$ con $k \in \mathbb{R}$  (costanti), quando esistono sono uguali fra loro, ottenendo come condizione necessaria:
\begin{center}
    $\lim_{x \to x_0} f(x) = l =\lim_{y \to y_0} f(y)$
\end{center}
\newpage

\subsection{Continuità}
\textbf{73. Come si definisce {\color{red}la continuità} di un limite su un dominio?}
\begin{definizione}[$\epsilon-\delta$ con la continuità]Siano $f(x,y) = \mathbf{x}$, quindi $f:A=\mathbb{R}^2 \to \mathbb{R}$ (ovvero $f$ è funzione continua in $A=\mathbb{R}^2$). Per dimostarlo devo mostrare che sia continua in ogni punto $\forall P_0=(x_0,y_0) \in \mathbb{R}^2$, dobbiamo dimostrare che $\lim_{{x,y} \to x_0,y_0} f(x,y) = f(x_0,y_0)$.
\end{definizione}
\textbf{\color{thmcolor}DIMOSTRAZIONE}

Si dimostra che $\forall \epsilon>0, \exists\delta = \delta(\epsilon)$ tale che $|f(x,y)-f(x_0,y_0)|< \epsilon$.

Sia $f(x,y) = \mathbf{x}$, vogliamo dimostrare che $f$ è continua in ogni punto $P_0 = (x_0, y_0) \in \mathbb{R}^2$. 

Fissato $\epsilon > 0$, dobbiamo trovare un $\delta > 0$ tale che la distanza tra le immagini sia minore di $\epsilon$. Osserviamo che:
\[ |f(x,y) - f(x_0,y_0)| = |\mathbf{x} - \mathbf{x}_0| \]
Quindi, possiamo scrivere che la distanza (con proprietà della positività indotta dalla norma):
\[ 0 \leq |\mathbf{x} - \mathbf{x}_0| = \sqrt{(\mathbf{x}-\mathbf{x}_0)^2} \leq \sqrt{(x-x_0)^2 + (y-y_0)^2} \]
Poiché la distanza euclidea $d(P, P_0) = \sqrt{(x-x_0)^2 + (y-y_0)^2}$, se poniamo \textbf{$\delta = \epsilon$}, otteniamo:
\[ d(P, P_0) < \delta \implies |\mathbf{x} - \mathbf{x}_0| < \delta = \epsilon \]
La condizione è soddisfatta, pertanto la funzione è continua in $\mathbb{R}^2$.
\hfill{\color{thmcolor}$\square$}

\begin{figure}[H]
\centering
\begin{tikzpicture}[scale=1.5]
    
    % Assi cartesiani
    \draw[->, thick] (-0.5,0) -- (4,0) node[right] {$x$};
    \draw[->, thick] (0,-0.5) -- (0,3.5) node[above] {$y$};
    
    % Punto P_0 = (x_0, y_0)
    \coordinate (P0) at (2,2);
    
    % Cerchio di raggio delta (intorno sferico)
    \draw[blue!70, thick] (P0) circle (1cm);
    \node[blue!70] at (2,3.15) {\small $B(P_0, \delta)$};
    
    % Raggio delta
    \draw[blue!70, thick, ->] (P0) -- (3,2);
    \node[blue!70, below] at (2.5,2) {$\delta$};
    
    % Punto generico P SUL bordo del cerchio
    \coordinate (P) at (2.707,2.707);
    \fill[green!60!black] (P) circle (1.5pt);
    \node[green!60!black, above right] at (P) {\small $P$};
    
    % Distanza da P a P_0
    \draw[green!60!black, dashed] (P0) -- (P);
    \node[green!60!black, left] at (2.55,2.55) {\small $= \delta$};
    
    % Punto rosso SOPRA la riga
    \fill[red] (P0) circle (2pt);
    \node[red, above left] at (P0) {\small $P_0$};
    
    % Proiezione su asse x
    \draw[dashed, gray] (P0) -- (2,0) node[below] {$x_0$};
    \draw[dashed, gray] (P) -- (2.707,0) node[below] {$x$};
    
    % Intervallo epsilon sull'asse x
    \draw[orange!80!red, very thick] (1,0) -- (3,0);
    \draw[orange!80!red, thick] (1,-0.1) -- (1,0.1);
    \draw[orange!80!red, thick] (3,-0.1) -- (3,0.1);
    \node[orange!80!red, below] at (1,-0.15) {$x_0 - \epsilon$};
    \node[orange!80!red, below] at (3,-0.15) {$x_0 + \epsilon$};
    
    % Freccia che mostra la distanza |x - x_0|
    \draw[<->, orange!80!red, thick] (2,-0.6) -- (2.707,-0.6);
    \node[orange!80!red, below] at (2.35,-0.6) {\tiny $|x-x_0| < \epsilon$};
    
    % Rettangolo verticale per mostrare la "proiezione"
    \draw[orange!80!red, dashed, opacity=0.3] (1,0) -- (1,3);
    \draw[orange!80!red, dashed, opacity=0.3] (3,0) -- (3,3);
    \fill[orange!80!red, opacity=0.1] (1,0) rectangle (3,3);
    
    % Annotazione chiave
    \node[align=center, font=\small] at (2,-1.5) {
        Se $\delta = \epsilon$, allora ogni punto dentro\\
        il cerchio ha $|x - x_0| < \epsilon$
    };
    
\end{tikzpicture}
\end{figure}

La continuità della proiezione $f(x,y)=\mathbf{x}$ esprime un fatto geometrico elementare: in un triangolo rettangolo, la lunghezza di un cateto ($|\mathbf{x}-\mathbf{x}_0|$) non può mai superare quella dell'ipotenusa ($d(P,P_0)$). 
Quindi, se limitiamo il movimento totale nel piano entro un raggio $\delta$, limitiamo automaticamente la variazione della funzione entro lo stesso valore. Non essendoci "salti" bruschi nelle coordinate mentre ci si sposta nel piano, la funzione è continua.\\

\textbf{74. Quando {\color{red}le funzioni sono continue}?}
\begin{teorema} Siano $f$ e $g$ due funzioni a più variabili continue a valori reali, allora potremmo dire che
\begin{itemize}
    \item $f \pm g$ e $f \cdot g$ sono continue;
    \item $\frac{f}{g}$ sono continue se solo se $g \not = 0$;
    \item $f^g$ è continua se solo se $g>0$, perché corrisponde a $e^{g \cdot ln(f)}$ dove $ln(f)$ è continua per $f>0$ che garantisce che sia ben definita, $g \cdot ln(f)$ è continua per il prodotto di funzioni continue ed $e$ è continua;
    \item $g \circ f$ è continua dove è definita, ossia valore assunto da f deve appartenere al dominio di g.
    \item I polinomi sono sempre continui su tutto $\mathbb{R}^n$ perché le funzioni coordinate $x_1, x_2, \ldots, x_n$ sono continue. Quindi, costruendo il polinomio a partire dalle coordinate tramite operazioni algebriche (somme, prodotti, multipli), otteniamo sempre una funzione continua.
\end{itemize}
\end{teorema}
Se abbiamo operazioni tra funzioni continue, il risultato è ancora continuo.


Questo teorema ci permette di affermare che le funzioni elementari, come polinomi a più variabili, funzioni razionali, funzioni esponenziali a più variabili, funzioni trigonometriche, sono tutte continue.

Alcuni esempi:
\begin{itemize}
    \item $f(x,y) = \sin(x^2+y^2)$ — continua su $\mathbb{R}^2$
    \item $f(x,y) = e^{xy}$ — continua su $\mathbb{R}^2$
    \item $f(x,y) = \ln(x^2 + y^2 + 1)$ — continua su $\mathbb{R}^2$ (argomento sempre positivo)
    \item $f(x,y) = \frac{x^2 - y^2}{x^2 + y^2 + 1}$ — continua su $\mathbb{R}^2$
    \item $f(x,y,z) = \sqrt{x^2 + y^2 + z^2}$ — continua su $\mathbb{R}^3$
    \item $f(x,y) = \arctan\left(\frac{y}{x}\right)$ — continua dove $x \neq 0$
    \item $f(x,y) = x^y$ con $x > 0$ — continua su $\{(x,y) : x > 0\}$
\end{itemize}

\subsection{Coordinate Polari}
\textbf{75. Cosa sono {\color{red}le coordinate polari}?}
\begin{definizione}[Coordinate polari] Un modo alternativo alle coordinate cartesiane per descrivere posizioni sul piano sono le coordinate polari, un sistema bidimensionale che identifica un punto $P$ attraverso due parametri:
la distanza $\rho = \sqrt{(x-x_0)^2+(y-y_0)^2}$ da un punto fisso $O$ (detto polo), e l'angolo $\theta$ formato da una semiretta fissa uscente da O (detta asse polare) e il segmento che unisce $O$ a $P$, misurato in senso antiorario.
\end{definizione}


\begin{figure}[H] 
    \centering
    \includegraphics[width=0.5\linewidth]{../Immagini/CoordinatePolari.png}
\end{figure}

Abbiamo specificato cos'è la distanza $\rho$ ma è \textbf{importante} e necessario spiegare $\theta$: geometricamente l'angolo $\theta$ è strettamente legato alla coefficente angolare (o pendenza) della retta che passa per il punto $P$, ossia quanto ci alziamo verticalmente rispetto a quanto ci spostiamo orizzontalmente.

Dalla trigonometria sappiamo che il coefficente angolare della retta è $\tan \theta = \frac{\sin \theta}{\cos \theta}$, che va a coincedere esattamente con la definizione di coefficente angolare dell'angolo $\theta$ delle coordinate polari.

\vspace{-1cm}
\begin{center}
    \[
    \tan \theta = \frac{\sin \theta}{\cos \theta} \text{ in coordinate polari } \Rightarrow \tan \theta = \frac{\rho \sin \theta}{\rho \cos \theta} = \frac{y_\rho}{x_\rho}
    \]
\end{center}
Quindi se conosciamo le coordinate polari $(x_\rho,y_\rho)$ possiamo trovare l'angolo $\theta$ invertendo la tangente e, sempre dalla trigonometria, sappiamo che $tan^{-1}= \arctan$: 
\vspace{-0.9cm}
\begin{center}
    \[
    \theta = \arctan \frac{y_\rho}{x_\rho}
    \]
\end{center}
\textbf{76. Come si studiano i limiti con {\color{red}le coordinate polari}?}

Per studiare il limite di una funzione $f(x,y)$ quando $(x,y) \rightarrow  (x_0,y_0)$, si effettua una trasformazione in coordinate polari centrata nel punto limite. 

Questo metodo è particolarmente efficace per funzioni radiali (che dipendono dalla distanza dal punto).

Procedimento:
\begin{enumerate}
    \item Si pone il centro del sistema polare in  $(x_0,y_0)$, il punto di accumulazione verso cui si tende, trasformando le coordinate cartesiane $(x,y)$ seguendo il sistema
    \[
        \begin{cases} 
             x = x_0 + \rho \cos(\theta) \\ 
             y = y_0 + \rho \sin(\theta) 
        \end{cases}
    \]
    \item Si studia il comportamento della nuova $f(\rho, \theta)$ per $\rho \rightarrow 0^+$, perché essendo una radice quadrata di una somma di quadrati, $\rho$ non può mai essere negativa, quindi si avvicina al centro resitingendosi verso $(x_0,y_0)$;
    \item Si verifica che il limite sia uniforme rispetto a $\theta$, ossia il valore non dipenda dalla direzione da cui ci si avvicina a $(x_0,y_0)$.
\end{enumerate}


\textbf{77. Perché è importante che il limite sia {\color{red}uniforme rispetto all'angolo} $\theta$?}

Come scritto sopra, il valore del limite \textbf{non} deve dipendere dalla direzione da cui ci si avvicina.
\begin{definizione}[$\epsilon-\delta$] 
\[
\forall \epsilon > 0 \text{ } \exists \delta_\epsilon>0 \text{ tale che } \forall \rho \text{ con }0 < \rho < \delta \text{ e } \forall \theta \in [0, 2 \pi]\text{ si ha } |f(x_0 + \rho \cos(\theta), y_0 + \rho \sin(\theta))| < \epsilon \text{ } 
\]
\end{definizione}
$\theta$ rappresenta tutte le possibili direzioni da cui ci si avvicina, se il limite dipende da $\theta$ significa che per cammini diversi otteniamo valori diversi e per l'esistenza del limite i valori devono essere gli stessi qualsiasi sia la direzione di avvicnamento.

Quindi, se il limite per $f(\rho, \theta) = g(\theta)$ per $\rho \rightarrow 0^+$ vuol dire che $g(\theta)$ assume valori diversi al variare di $\theta$, violando l'uniformità rispetto a $\theta$ e affermando che il limite non esiste.\\

\textbf{78. Come si può usare {\color{red}il teorema del confronto} per dimostrare l'esistenza di un limite di una funzione multivariata?}
\begin{definizione}[Il teorema del confronto (o dei due carabinieri)] Siano $f(x,y), g(x,y), h(x,y)$ tre funzioni con un intorno di $P_0 = (x_0, y_0)$. Sia
\[
 f(x,y)\leq g(x,y) \leq h(x,y)
\]
Se il limite di $f(x,y)$ e $h(x,y)$ tende $l$ quando $(x,y) \rightarrow (x_0, y_0)$ allora il limite di $g(x,y)$ tende $l$ quando $(x,y) \rightarrow (x_0, y_0)$.
\end{definizione}
Questo perché la funzione centrale $g(x,y)$ è "intrappolata" tra la due, quindi il valore del suo limite dovrà per forza tendere a $l$.\\

Quindi, l'esistenza del limite è garantita se è possibile individuare una funzione maggiorante $g(\rho)$ tale che:
\[
|f(x_0 + \rho \cos \theta, y_0 + \rho \sin \theta) - \ell| \leq g(\rho)
\]
Affinché tale condizione sia valida, la funzione $g(\rho)$ deve possedere tre caratteristiche fondamentali:
\begin{itemize}
    \item \textbf{Indipendenza da $\theta$ (Funzione Radiale):} $g$ deve dipendere esclusivamente dalla distanza $\rho$. Questo garantisce che il valore del limite non dipenda dalla direzione scelta per avvicinarsi al punto $(x_0, y_0)$, assicurando l'uniformità della convergenza rispetto a $\theta$;
    \item \textbf{Non negatività:} Deve risultare $g(\rho) \geq 0$, in quanto deve maggiorare un valore assoluto (che rappresenta una distanza geometrica tra il valore della funzione e il suo limite);
    \item \textbf{Natura infinitesima:} Deve essere verificato che $\lim_{\rho \to 0^+} g(\rho) = 0$.
\end{itemize}
La necessità di una funzione $g(\rho)$ non negativa e infinitesima risiede nell'applicazione del \textbf{Teorema del Confronto (o dei Carabinieri)}. 

Sapendo che il valore assoluto di una differenza è per definizione non negativo, possiamo impostare la seguente catena di disuguaglianze:
\[
0 \leq |f(x_0 + \rho \cos \theta, y_0 + \rho \sin \theta) - \ell| \leq g(\rho)
\]
Se la funzione maggiorante $g(\rho)$ tende a zero per $\rho \to 0^+$, allora anche il termine centrale è costretto a tendere a zero. Formalmente:
\begin{enumerate}
    \item Si maggiora lo scarto: $|f(\rho, \theta) - \ell| \leq g(\rho)$.
    \item Si verifica il comportamento al limite $\lim_{\rho \to 0^+} g(\rho) = 0$.
    \item Per il Teorema dei Carabinieri, si conclude che:
    \[
    \lim_{\rho \to 0^+} |f(x_0 + \rho \cos \theta, y_0 + \rho \sin \theta) - \ell| = 0
    \]
\end{enumerate}
Tale risultato implica univocamente che $f \to \ell$, confermando l'esistenza del limite indipendentemente dalla direzione $\theta$ con cui ci si avvicina al punto $(x_0, y_0)$.\\

\textbf{79. Come si {\color{red}studia la continuità} di una funzione in due variabili?}

Sia $f: A \subseteq \mathbb{R}^2 \to \mathbb{R}$ e sia un punto $P = (x_0,y_0)$, allora $f$ è continua in $P$ se rispetta le condizioni:
\begin{itemize}
    \item $f$ sia definita in $P  = (x_0,y_0) \in D$ dominio di $f$;
    \item Esiste il limite $\lim_{(x,y) \to (x_0,y_0)} f(x,y) = \ell$ e $\ell = f(x_0,y_0)$
\end{itemize}
Il procedimento meccanico per dimostrare la continuità che coinvolge anche le coordinate polari:
\begin{itemize}
    \item $P  = (x_0,y_0) \in D$ sia definita nel dominio di $f$;
    \item Si trasforma le coordinate cartesiane in coordinate polari, si controlla se esiste il limite $\lim_{\rho \to 0^+} f(\rho, \theta) = 0$ e con le dovute analisi di maggiorazione si controlla che il limite dipenda uniformemente rispetto a $\theta$ verificando l'uguaglianza.
\end{itemize}

\section{Derivata}
\textbf{80. Per la {\color{red}derivabilità e differenziabilità}, quali sono le principali {\color{red}differenze concenttuali} rispetto al caso unidimensionale?}

Se $f:I \subseteq \mathbb{R} \rightarrow \mathbb{R}$ si ha che la derivata $f'(x)$ o $\frac{\text{d}f(x)}{\text{d}x}$ rappresenta il tasso di variazione di $f$ in un punto. Matematicamente $f'(x)$ è definito come, se esiste finito, il limite 
\vspace{-0.8cm}
\begin{center}
    \[
    \lim_{h \rightarrow 0} \frac{f(x+h)-f(x)}{h} \text{ } \forall x \in \mathbb{R}
    \]
\end{center}
Quindi, $f'(x)$ è il coefficente angolare della retta tangente al grafico di $f(x)$ nel punto $P=(x,f(x))$.
\begin{figure}[H] 
    \centering
    \includegraphics[width=0.5\linewidth]{../Immagini/DerivataUnidimensionale.jpg}
\end{figure}
Nel caso delle funzioni multivariate le direzioni degli assi coordinati ci permettono di definire le derivate parziali di f (il numero di derivate parziali è dato dal numero di variabili):
\begin{itemize}
    \item Esistono infinite direzioni lungo cui la funzione può variare;
    \item Le derivate parziali non sono più sufficienti a garantire la differenziabilità;
    \item Una funzione può avere tutte le derivate parziali in un punto senza essere continua o differenziabile in quel punto. Serve una condizione più forte: l'esistenza di un'approssimazione lineare con resto;
    \item Il concetto di gradiente sarà importante per comprendere le direzioni di variazione di una funzione.
\end{itemize}

\subsection{Derivata parziale}
\textbf{81. Cos'è {\color{red}una derivata parziale}?}
\begin{definizione}[Derivata parziale rispetto ad una variabile] La derivata parziale di una funzione $f:\mathbb{R}^n \rightarrow \mathbb{R}$ rispetto ad una variabile $k$ è la derivata ordinaria di $f$ considerata come funzione delle sola variabile $k$, mantenendo "fisse" le altre variabili. 
\end{definizione}
Il termine "parziale" indica che si sta considerando solo una parte del dominio, ovvero la variazione della funzione rispetto a una singola variabile, "congelando" tutte le altre. È una derivata parziale nel senso che cattura solo un aspetto del comportamento della funzione, non la sua variazione completa in tutte le direzioni.

La derivata parziale fornisce informazioni locali lungo una direzione specifica (quella dell'asse coordinata corrispondente), ma non descrive il comportamento globale della funzione in un intorno del punto.\\

Formalmente, sia un punto $P_0=(x_0,y_0) \in A$ e sia $f:A \subseteq \mathbb{R}^2 \rightarrow \mathbb{R}$ con $A$ aperto:
\begin{itemize}
    \item La derivata rispetto a x (solo x incrementa)
    \vspace{-0.8cm}
    \begin{center}
        \[
        \lim_{h \rightarrow 0} \frac{f(x_0+h,y_0)-f(x_0,y_0)}{h} 
        \]
    \end{center}
    \item La derivata rispetto a y (solo y incrementa)
    \vspace{-0.8cm}
    \begin{center}
        \[
        \lim_{h \rightarrow 0} \frac{f(x_0,y_0+h)-f(x_0,y_0)}{h}
        \]
    \end{center}
\end{itemize}
Se questi due limiti esistono finiti, si dirà che $f$ è derivabile parzialmente (o localmente) rispetto a $x$ e a $y$ usando la notazione $\frac{\partial f}{\partial x_0}(x_0,y_0)$ e $\frac{\partial f}{\partial y_0}(x_0,y_0)$.\\

Potremmo dire che la derivata parziale è una derivata ordinaria di $f$ sulle tracce parallele agli assi delle coordinate, lungo il piano o $x=x_0$ o $y=y_0$:

\begin{itemize}
    \item $\frac{\partial f}{\partial x}(x_0,y_0)= g'(x_0)=\frac{\text{d}}{\text{d}x}f(x,y_0)$
    \item  $\frac{\partial f}{\partial y}(x_0,y_0)= h'(y_0)=\frac{\text{d}}{\text{d}y}f(x_0,y)$
\end{itemize}

\begin{definizione}[Derivata parziale rispetto a n variabili] Sia $f:A \subseteq \mathbb{R}^2 \rightarrow \mathbb{R}$ con $A$ aperto e sia un punto $P_0=(\mathbf{x}) \in A$. La derivata parziale della $f$ rispetto alla variabile $x_i$ con $i = 1...n$ in $P_0=(\mathbf{x})=(x_1...x_n)$ corrisponde a 
\[
        \lim_{h \rightarrow 0} \frac{f(x_1,x_2,...,x_{i-1},x_{i+h},...,x_n)-f(x_1,...,x_n)}{h}
\]
\end{definizione}  
Se tale limite esiste ed è finito, esiste la derivata parziale e si scrive come:
\[
\frac{\partial f}{\partial x_i}(\mathbf{x})=f_{x_i}(\mathbf{x})=D_{x_i}f(\mathbf{x})
\]
\subsection{Gradiente}
\textbf{82. Quali {\color{red}condizioni} devono essere soddisfatte perché {\color{red}una funzione sia derivabile} in un punto e su un aperto?}

Una funzione $f:A \subseteq \mathbb{R}^2 \rightarrow \mathbb{R}$ con $A$ aperto è derivabile su un punto $P_0=(x_0,y_0) \in A$ se esistono le derivate parziali $\frac{\partial f}{\partial x_0}(x_0,y_0)$ e $\frac{\partial f}{\partial y_0}(x_0,y_0)$, o meglio, se esiste il \textbf{vettore} gradiente $\nabla f(x_o,y_0)=(\frac{\partial f}{\partial x}(x_0,y_0), \frac{\partial f}{\partial y}(x_0,y_0))$ (nel caso generale $\nabla f(\mathbf{x})=(\frac{\partial f}{\partial x_1}(\mathbf{x}),..., \frac{\partial f}{\partial x_n}(\mathbf{x}))$)\\

Affinché una funzione multivariata sia derivabile in un punto:
\begin{itemize}
    \item $f$ deve essere continua nel punto;
    \item Devono esistere tutte le derivate nel punto, quindi deve esistsere il gradiente tale che $\nabla f(\mathbf{x})=(\frac{\partial f}{\partial x_1}(\mathbf{x}),..., \frac{\partial f}{\partial x_n}(\mathbf{x}))$.
\end{itemize}
\textbf{83. Perché si parla di vettori e cos'è {\color{red}il vettore gradiente}?}

Avendo più direzioni lungo cui la funzione può variare dobbiamo "memorizzare" $n$ tassi di variazione, di conseguenza le $n$ derivate parziali vengono organzzate in un vettore n-dimensionale che contiene tutta l'informazione sulla variazione locale della funzione.

\begin{definizione}[Gradiente] E' un vettore denotato $\nabla f \in \mathbb{R}^n$ le cui componenti sono le derivate parziali della funzione $f$ nel punto $\mathbf{x}$
\[
\nabla f({\mathbf{x}}) = \begin{pmatrix} 
\frac{\partial f}{\partial x_1}({\mathbf{x}}) \\ 
\frac{\partial f}{\partial x_2}({\mathbf{x}}) \\ 
\vdots \\ 
\frac{\partial f}{\partial x_n}({\mathbf{x}}) 
\end{pmatrix} \in \mathbb{R}^n
\]
\end{definizione}
Esso raccoglie tutte le derivate parziali e misura quanto $f$ è "sensibile" a variazioni in ciascuna variabile.\\

Da qui in poi non abbiamo più una retta tangente ma un \textbf{piano tangente}, ovvero un'approssimazione lineare tramite il prodotto scalare $f(\mathbf{x}+h) \approx f(\mathbf{x})+<\nabla f(\mathbf{x}),\mathbf{h}>+o(h)$
\begin{itemize}
    \item \textbf{In una variabile}, vicino ad un punto $x_0$ vale l'approssimazione lineare $f(x_o+h) \approx f(x_0)+f'(x_0)h$ con $h$ una piccola di $x$, $f'(x_0)$ la pendenza della tangente e $f'(x_0)h$ di quanto cambia $f$ quando $x$ cambia di $h$.
    \item \textbf{In più variabili} il punto non si tratta più di un numero ma di un vettore $\mathbf{x} = (x_0,x_1,...,x_n)$ e anche la variazione non è più un numero ma un vettore $\mathbf{h}=(h_1,...,h_n)$.

    Si definisce il prodotto scalare perché quando ti sposti da $\mathbf{x}$ a $\mathbf{x}+h$ stai cambiando tutte le variabili della funzione insieme, ossia sommando il contributo di ogni variabile per la corrispettiva derivata parziale
\[
<\nabla f(\mathbf{x}),\mathbf{h}> = \sum_{i=1}^n \frac{\partial f}{\partial x_i}(\mathbf{x}) \cdot h_i
\]
Quindi, $f(\mathbf{x}+h) \approx f(\mathbf{x})+<\nabla f(\mathbf{x}),\mathbf{h}>$ vicino $\mathbf{x}$ la funzione si comporta come un piano tangente, il termine con il prodotto scalare approssima linearmente nel modo migliore e l'errore diventa trascurabile quando $h$ è piccolo.\\
\end{itemize}

\textbf{84. Come si {\color{red} rappresenta geometricamente} una derivata parziale?}

Geometricamente, nelle funzioni a due variabili le derivate parizlai sono legate al concetto di rette tangenti al grafico dell curve ottenute fissata la traccia $xz$ o $yz$. 

\begin{figure}[H] 
    \centering
    \includegraphics[width=0.6\linewidth]{../Immagini/DerivataParzialeGEOMETRICAMENTE.png}
\end{figure}

\textbf{85. La derivata parziale è un concetto particolare della derivata direzionale, cosa si intende per {\color{red}derivata parziale in un punto usando la notazione vettoriale}?}

Sia $f: A \subseteq\mathbb{R}^n \rightarrow \mathbb{R}$ e siano $\mathbf{x}=\{x_1,...,x_n\} \in A$ e $e_i$ versore componenente della base canonica $e = (e_1,...,e_n)$ di $\mathbb{R}^n$ (direzione nello spazio).

La derivata direzionale rispetto a $f$ rispetto alla direzione di $e_i$ nel punto $\mathbf{x}$ se esiste allora esiste finito
\[
  \lim_{h \rightarrow 0} \frac{f(\mathbf{x}+\mathbf{h}e_i)-f(\mathbf{x})}{h} = \frac{\partial f}{\partial x_i}(\mathbf{x})
\]
\begin{itemize}
    \item $he_i = h(0...1...0)=(0...h...0)$
    \item $\mathbf{x}+he_i = (x_i...x_i+h...x_n)$
    \item $f$ dipende da $h$
\end{itemize}
La derivata parziale  può essere rappresentata con un concetto più generale: la \textbf{derivata direzionale}.
Dato l'iperpiano tangente, per il punto in cui si poggia il piano passano infinite rette.

Informalmente, la derivata direzionale è la pendenza di una di quelle rette in riferimento ad una direzione scelta partendo dal punto del dominio.

\begin{definizione}[Derivata direzionale] Dato un versore $\mathbf{v}$, ovvero un vettore di norma unitaria ($||v||=1$), in $\mathbb{R}^n$, sia $f:A\subseteq \mathbb{R}^n \rightarrow \mathbb{R}$ con $A$ aperto e sia $\mathbf{x} = (x_1...x_n) \in A$.

La derivata direzionale di $f$ rispetto alla direzione data da $\mathbf{v}$ nel punto $\mathbf{x}$ esiste se esiste ed è finito il limite
\[
 \lim_{h \rightarrow 0} \frac{f(\mathbf{x}+h\mathbf{v})-f(\mathbf{x})}{h} = \frac{\partial f}{\partial {\vec{v}}}(\mathbf{x})
\]
\end{definizione}

L'uso del versore risulta importante perché non interessa la lunghezza del vettore ma solamente la direzione e il verso, quindi se non è di norma unitaria và normalizzato con la formula $v=\frac{v}{||v||}$.

\section{Differenziabilità}
\textbf{86. Qual è la {\color{red}differenza tra differenziabilità e derivabilità}?}
\begin{itemize}
\item \textbf{Analisi 1 (Caso unidimensionale)}:
    
Per le funzioni di una variabile, non c'è differenza tra derivabilità e differenziabilità.

La derivabilità garantisce l'esistenza della retta tangente al grafico della funzione in un punto $x_0$.

La differenziabilità garantisce qualcosa di più forte: la funzione può essere sostituita localmente con la sua retta tangente.

In modo più preciso, se $f$ è differenziabile in $x_0$, allora:
\[
f(x) = f(x_0) + f'(x_0)(x - x_0) + o(x - x_0)
\]

dove il termine $o(x - x_0)$ indica una quantità che tende a zero più velocemente di $x - x_0$.

Questo significa che:
\[
f(x) - \text{retta tangente} = o(x - x_0)
\]

e quindi, avvicinandosi a $x_0$, la funzione e la sua approssimazione lineare diventano sempre più indistinguibili, indipendentemente da quanto piccolo sia l'intorno scelto.

\item \textbf{Analisi 2 (Caso multivariato)}:

A differenza del caso unidimensionale, in più variabili i concetti di derivabilità (parziale) e differenziabilità si separano nettamente. La semplice esistenza delle derivate parziali (lungo gli assi) è una condizione debole: garantisce l'esistenza delle tangenti solo lungo direzioni specifiche, ma non implica la continuità né l'esistenza di un piano tangente coerente.

Se $f(\mathbf{x})$ è differenziabile in $\mathbf{x}_0 = (x_0, y_0)$, allora:
\[
f(\mathbf{x}) = f(\mathbf{x}_0) + \nabla f(\mathbf{x}_0) \cdot (\mathbf{x} - \mathbf{x}_0) + o(\|\mathbf{x} - \mathbf{x}_0\|)
\]
Esplicitamente in due variabili:
\[
f(x,y) = f(x_0, y_0) + f_x(x_0, y_0)(x - x_0) + f_y(x_0, y_0)(y - y_0) + o\!\left(\sqrt{(x - x_0)^2 + (y - y_0)^2}\right)
\]
Il termine di resto $o(\|\mathbf{x} - \mathbf{x}_0\|)$ è più piccolo della distanza euclidea tra i punti, ovvero:
\[
\lim_{\mathbf{x} \to \mathbf{x}_0} \frac{f(\mathbf{x}) - f(\mathbf{x}_0) - \nabla f(\mathbf{x}_0) \cdot (\mathbf{x} - \mathbf{x}_0)}{\|\mathbf{x} - \mathbf{x}_0\|} = 0
\]

Le derivate parziali forniscono informazioni solo lungo $n$ direzioni privilegiate (gli assi coordinati). Senza differenziabilità, non c'è garanzia che queste informazioni si ``assemblino'' in un piano tangente coerente: la superficie potrebbe avere comportamenti irregolari nelle direzioni oblique, con una ``raggiera'' di pendenze che non giacciono su un unico piano ben definito.

La differenziabilità richiede che la funzione possa essere \textit{realmente approssimata} dal piano tangente in tutte le direzioni, non solo lungo gli assi.

Consideriamo ad esempio la funzione:
\[
f(x,y) = \sqrt{x^2 + y^2}
\]
che rappresenta un cono con vertice nell'origine. Nel punto $(0,0)$:
\begin{itemize}
    \item Le derivate parziali esistono: $f_x(0,0) = f_y(0,0) = 0$
    \item Tuttavia la funzione \textbf{non è differenziabile} in $(0,0)$ perché mantiene la forma ``a punta'' del cono in ogni intorno del vertice: non può essere approssimata da un piano tangente, qualunque sia la dimensione dell'intorno considerato.
\end{itemize}

Una funzione $f: A \subseteq \mathbb{R}^n \to \mathbb{R}$ è \textit{differenziabile} in $\mathbf{x}_0$ se l'incremento della funzione può essere approssimato linearmente in modo globale.
Esiste cioè un'applicazione lineare $L: \mathbb{R}^n \to \mathbb{R}$ tale che:
\[
    f(\mathbf{x}_0 + \mathbf{h}) = f(\mathbf{x}_0) + L(\mathbf{h}) + o(\|\mathbf{h}\|) \quad \text{per } \mathbf{h} \to \mathbf{0}
\]
Se $f$ è differenziabile, l'applicazione lineare $L$ è univocamente determinata dal gradiente: $L(\mathbf{h}) = \nabla f(\mathbf{x}_0) \cdot \mathbf{h}$.

In termini geometrici, il valore della funzione in un punto vicino $\mathbf{x} = \mathbf{x}_0 + \mathbf{h}$ è composto da tre parti:
\begin{itemize}
    \item \textbf{Il punto di partenza:} $f(\mathbf{x}_0)$, la quota base.
    \item \textbf{La variazione lineare (Piano Tangente):} $L(\mathbf{h}) = \sum_{i=1}^n \frac{\partial f}{\partial x_i}(\mathbf{x}_0) h_i$. Questa parte rappresenta lo spostamento che avremmo se ci muovessimo sul piano tangente invece che sulla superficie curva.
    \item \textbf{L'errore di approssimazione:} $o(\|\mathbf{h}\|)$. È lo scarto tra la superficie reale e il piano tangente. La differenziabilità impone che questo errore diventi trascurabile molto più velocemente della distanza dal punto di contatto.
\end{itemize}

\textbf{Condizione sufficiente per la differenziabilità (Teorema del Differenziale Totale):}

Se tutte le derivate parziali di $f$ esistono e sono continue in un intorno di $\mathbf{x}_0$, allora $f$ è differenziabile in $\mathbf{x}_0$. Questa è una condizione sufficiente (ma non necessaria) molto utile nella pratica.

Grazie alla differenziabilità, possiamo usare il differenziale per stimare il valore della funzione in punti vicini a $\mathbf{x}_0$ senza dover ricalcolare la funzione complessa. Linearizzando $\mathbf{x} = \mathbf{x}_0 + \mathbf{h}$ si ottiene:
\[
f(\mathbf{x}) \approx f(\mathbf{x}_0) + \nabla f(\mathbf{x}_0) \cdot \mathbf{h}
\]
\end{itemize}
\newpage

\textbf{87. Quindi, come si definisce {\color{red}una funzione differenziabile}?}
 \begin{definizione}[Differenziabilità] Una funzione $f: A \subseteq \mathbb{R}^n \to \mathbb{R}$ è differenziabile in $\mathbf{x}_0 \in A$ se e solo se: 
 \begin{itemize}
     \item $f$ è derivabile in $\mathbf{x}_0$, ovvero $\exists \nabla f(\mathbf{x}_0)$;
     \item Vale il seguente limite:
     \[
    \lim_{\mathbf{h} \to \mathbf{0}} \frac{f(\mathbf{x}_0 + \mathbf{h}) - f(\mathbf{x}_0) - \nabla f(\mathbf{x}_0)\cdot \mathbf{h}}{\|\mathbf{h}\|} = 0
    \]
\end{itemize}
Dove $\mathbf{h} = (h_1, \dots, h_n)$ è il vettore incremento e $\|\mathbf{h}\| = \sqrt{\sum h_i^2}$ è la sua norma euclidea.
\end{definizione}
Affermare che il limite è 0 significa che il numeratore (l'errore dell'approssimazione lineare) tende a zero con un \textbf{ordine di infinitesimo superiore} rispetto al denominatore (la distanza percorsa $\|\mathbf{h}\|$). In simboli:
\[
f(\mathbf{x}_0 + \mathbf{h}) - f(\mathbf{x}_0) - \nabla f(\mathbf{x}_0)\cdot \mathbf{h} = o(\|\mathbf{h}\|)
\]

\newpage
\textbf{88. Perché la differenziabilità in un punto è legata all'{\color{red}esistenza del piano tangente} al grafico della funzione nel punto?}

Geometricamente, dire che una funzione $f: \mathbb{R}^2 \rightarrow\mathbb{R}$ è differenziabile in un punto $P_0=(x_0,y_0)$ equivale a dire che il grafico della funzione ammetta un iperpiano tangente \textbf{non verticale} in $P_0=(\mathbf{x}_0, f(\mathbf{x}_0))$.

Non basta che il piano tocchi il punto (quello lo fanno infiniti piani). Significa che il piano è talmente "aderente" alla superficie curva che, se "zoommiamo" infinitamente sul punto $P_0$, la superficie curva del grafico diventa indistinguibile dal piano piatto. L'errore che commettiamo sostituendo la superficie con il piano tende a zero più velocemente della distanza dal punto.



\begin{figure}[H] 
    \centering
    \includegraphics[width=0.6\linewidth]{../Immagini/IperpianoTangente.png}
    \end{figure}

\textbf{89. Come si ottiene {\color{red}l'equazione del piano tangente}?}

Sia $f: A \subseteq \mathbb{R}^2 \to \mathbb{R}$ differenziabile in $P_0=(x_0,y_0)$. 
L'equazione del \textbf{piano tangente} al grafico in $P_0$ è:
\[
    z = f(x_0,y_0) + \nabla f(P_0) \cdot \mathbf{h}
\]
Dove $\mathbf{h} = (x-x_0, y-y_0)$ è il vettore spostamento. 
Esplicitando in componenti:
\[
    z = \underbrace{f(x_0,y_0)}_\text{punto di contatto} + \underbrace{\frac{\partial f}{\partial x}(P_0)(x - x_0) + \frac{\partial f}{\partial y}(P_0)(y - y_0)}_\text{inclinazione}
\]

Questa formula è la naturale estensione geometrica della retta tangente. 
In dimensione 1, l'equazione della retta tangente è:
\[
    y = f(x_0) + f'(x_0)(x-x_0)
\]
Mentre l'approssimazione della funzione (formula di Taylor al primo ordine) è:
\[
    f(x) = \underbrace{f(x_0) + f'(x_0)(x-x_0)}_{\text{Parte Lineare (Retta)}} + \underbrace{o(x-x_0)}_{\text{Errore}} \quad \text{per } x \to x_0
\]
Allo stesso modo, in $\mathbb{R}^2$, il piano tangente rappresenta la parte lineare dello sviluppo della funzione.\\

\textbf{90. Qual è il {\color{red}significato geometrico del differenziale}?}

Dato un punto base $\mathbf{x}_0$ e un vettore spostamento $\mathbf{h}$:

\begin{itemize}
    \item \textbf{L'incremento vero ($\Delta f$):} 
    Rappresenta la variazione reale della funzione muovendosi sulla superficie curva del grafico.
    \[
        \Delta f = f(\mathbf{x}_0 + \mathbf{h}) - f(\mathbf{x}_0)
    \]
    
    \item \textbf{Il differenziale ($df$):} 
    Rappresenta la variazione calcolata muovendosi sul \textit{piano tangente}. È la parte lineare dell'incremento.
    \[
        df = \nabla f(\mathbf{x}_0) \cdot \mathbf{h}
    \]
\end{itemize}
Mentre $\Delta f$ misura il dislivello esatto sulla "montagna" (il grafico di $f$), il differenziale $df$ misura il dislivello che avremmo se camminassimo sul piano tangente che approssima la montagna in quel punto.

Per spostamenti $\mathbf{h}$ molto piccoli (in un intorno di $\mathbf{x}_0$), l'errore commesso sostituendo la curva con il piano è trascurabile, quindi:
\[
    \Delta f \approx df
\]
Più formalmente, la differenza $\Delta f - df$ è un infinitesimo di ordine superiore rispetto alla norma dello spostamento ($o(\|\mathbf{h}\|)$).\\

\textbf{91. Se è {\color{red}differenziabile} allora è {\color{red}continua}?}

Sia $f: A \subseteq \mathbb{R}^n \rightarrow\mathbb{R}$ differenziabile in un punto $\mathbf{x}_0\in A$ allora è continua in $\mathbf{x}_0$.\\

\textbf{\color{thmcolor}DIMOSTRAZIONE}

Per dimostrare la continuità in $\mathbf{x}_0$, dobbiamo verificare che:
\[
\lim_{\mathbf{h} \to \mathbf{0}} f(\mathbf{x}_0+\mathbf{h}) = f(\mathbf{x}_0)
\]

Dalla definizione di differenziabilità, possiamo scrivere l'incremento della funzione come somma di una parte lineare e un infinitesimo di ordine superiore:
\[
f(\mathbf{x}_0 +\mathbf{h}) = f(\mathbf{x}_0) + \nabla f(\mathbf{x}_0) \cdot \mathbf{h} + o(\|\mathbf{h}\|) \quad \text{per } \mathbf{h} \to \mathbf{0}
\]

Passando al limite per $\mathbf{h} \to \mathbf{0}$ in entrambi i membri:
\[
\lim_{\mathbf{h} \to \mathbf{0}} f(\mathbf{x}_0+\mathbf{h}) = \lim_{\mathbf{h} \to \mathbf{0}} \left[ f(\mathbf{x}_0) + \nabla f(\mathbf{x}_0) \cdot \mathbf{h} + o(\|\mathbf{h}\|) \right]
\]

Sfruttando la linearità del limite (il limite della somma è la somma dei limiti), analizziamo i tre termini separatamente:

\begin{itemize}
    \item Il primo termine è costante rispetto ad $\mathbf{h}$ (non dipende dall'incremento), quindi:
    \[
    \lim_{\mathbf{h} \to \mathbf{0}} f(\mathbf{x}_0) = f(\mathbf{x}_0)
    \]

    \item Il termine lineare (prodotto scalare) tende a $0$. Possiamo dimostrarlo usando la disuguaglianza di Cauchy-Schwarz e il teorema del confronto. Notiamo che:
    \[
    0 \leq |\nabla f(\mathbf{x}_0) \cdot \mathbf{h}| \leq \|\nabla f(\mathbf{x}_0)\| \cdot \|\mathbf{h}\|
    \]
    Poiché $\|\nabla f(\mathbf{x}_0)\|$ è un numero finito e $\|\mathbf{h}\| \to 0$, il prodotto tende a zero. Di conseguenza:
    \[
    \lim_{\mathbf{h} \to \mathbf{0}} \left( \nabla f(\mathbf{x}_0) \cdot \mathbf{h} \right) = 0
    \]

    \item L'ultimo termine tende a zero per la definizione stessa di o-piccolo (errore infinitesimo):
    \[
    \lim_{\mathbf{h} \to \mathbf{0}} o(\|\mathbf{h}\|) = 0
    \]
\end{itemize}

Mettendo tutto insieme otteniamo:
\[
\lim_{\mathbf{h} \to \mathbf{0}} f(\mathbf{x}_0+\mathbf{h}) = f(\mathbf{x}_0) + 0 + 0 = f(\mathbf{x}_0)
\]

Poiché il limite coincide con il valore della funzione nel punto, $f$ è continua in $\mathbf{x}_0$.

\hfill{\color{thmcolor}$\square$}
 \newpage
\textbf{92. Enunciare e dimostrare {\color{red}il teorema del differenziale}.}
\begin{teorema}[Teorema del differenziale] Sia $f: A \subseteq \mathbb{R}^n \rightarrow\mathbb{R}$ con $A$ aperto e si suppone che $f$ sia derivabile su tutto $A$ tale che
\[
\exists \nabla f(\mathbf{x}), \forall \mathbf{x} \in A \Leftrightarrow \text{esistono } f_{x_1} (\mathbf{x}), f_{x_2} (\mathbf{x}), \dots, f_{x_n} (\mathbf{x})
\]
Se le $n$ derivate sono continue allora $f$ è differenziabile nel punto $\mathbf{x}$.
\end{teorema}
\textbf{\color{thmcolor}DIMOSTRAZIONE} su $n = 2$ (vale per $n$ variabili)

Sia $f:A \subseteq \mathbb{R}^2 \to \mathbb{R}$ tale che $(x,y) \to f(x,y)$, si supppone che le derivate rispetto alle due variabili esistano $\forall(x,y)\in A$ esistano e siano continue, allora mostro che $f$ è differenziabile nel punto $(x,y), \forall\mathbf{h} \in \mathbb{R}^2$ con $\mathbf{h} = (h,k)$
\[
\lim_{h,k \to \mathbf{0}} \frac{f(x + h, y + k) - f(x, y) - \nabla f(x,y)\cdot (h,k)}{\sqrt{h^2+k^2}} = 0
\]

\textbf{{\color{thmcolor}Passo 1}}: Consideriamo l'incremento della funzione:
\[
\Delta f = f(x+h, y+k) - f(x,y)
\]
Aggiungiamo e sottraiamo il termine misto $f(x, y+k)$
\[
= \underbrace{[f(x+h, y+k) - f(x, y+k)]}_{\text{Variazione solo in } x} + \underbrace{[f(x, y+k) - f(x,y)]}_{\text{Variazione solo in } y}
\]
Perché nel primo membro la $y$ in $y+k$ rimane costante, nel secondo membro invece è la $x$ ad essere costante.\\

\textbf{{\color{thmcolor}Passo 2}}: Applicazione del Teorema di Lagrange

Poiché $f$ è derivabile, applichiamo il Teorema di Lagrange (del valor medio) separatamente alle due parentesi:

\begin{itemize}
    \item Per la prima parentesi (variabile $x$), esiste un punto $x_1$ compreso tra $x$ e $x+h$ tale che:
    \[
    f(x+h, y+k) - f(x, y+k) = f_x(x_1, y+k) \cdot (x+h-x) = f_x(x_1, y+k) \cdot h \text{ con } h >0
    \]
    
    \item Per la seconda parentesi (variabile $y$), esiste un punto $y_1$ compreso tra $y$ e $y+k$ tale che:
    \[
    f(x, y+k) - f(x,y) = f_y(x, y_1) \cdot (y+k-y) = f_y(x, y_1) \cdot k \text{ con } k >0
    \]
\end{itemize}
Quindi l'incremento diventa:
\[
\Delta f = f_x(x_1, y+k)h + f_y(x, y_1)k
\]

\textbf{{\color{thmcolor}Passo 3}}: Costruzione del limite di differenziabilità

Dobbiamo verificare che:
\[
\lim_{(h,k) \to (0,0)} \frac{f(x+h, y+k) - f(x,y) - [f_x(x,y)h + f_y(x,y)k]}{\sqrt{h^2+k^2}} = 0
\]
Sostituiamo $\Delta f$ con l'espressione trovata con Lagrange. Il numeratore diventa:
\[
\text{Num} = [f_x(x_1, y+k)h + f_y(x, y_1)k] - [f_x(x,y)h + f_y(x,y)k]
\]
Raccogliamo $h$ e $k$:
\[
\text{Num} = [f_x(x_1, y+k) - f_x(x,y)]h + [f_y(x, y_1) - f_y(x,y)]k
\]

\textbf{{\color{thmcolor}Passo 4}}: Maggiorazione con il valore assoluto

Passiamo al valore assoluto e usiamo la disuguaglianza triangolare ($|a+b| \le |a|+|b|$):
\[
\left| \frac{\text{Num}}{\sqrt{h^2+k^2}} \right| \le \frac{|f_x(x_1, y+k) - f_x(x,y)| \cdot |h|}{\sqrt{h^2+k^2}} + \frac{|f_y(x, y_1) - f_y(x,y)| \cdot |k|}{\sqrt{h^2+k^2}}
\]
Osserviamo che i rapporti $\frac{|h|}{\sqrt{h^2+k^2}}$ e $\frac{|k|}{\sqrt{h^2+k^2}}$ sono sempre $\le 1$ (poiché il cateto è minore dell'ipotenusa).
Quindi:
\[
0 \le \text{Limite} \le |f_x(x_1, y+k) - f_x(x,y)| + |f_y(x, y_1) - f_y(x,y)|
\]

\textbf{{\color{thmcolor}Passo 5}}: Conclusione per continuità

Facciamo tendere $(h,k) \to (0,0)$.
\begin{itemize}
    \item Se $h \to 0$, allora $x_1 \to x$ (perché compreso tra $x$ e $x+h$).
    \item Se $k \to 0$, allora $y_1 \to y$.
\end{itemize}
Poiché per ipotesi $f$ è di classe $C^1$, le derivate parziali $f_x$ e $f_y$ sono \textbf{continue}.
Pertanto:
\[
\lim_{(h,k) \to (0,0)} f_x(x_1, y+k) = f_x(x,y)
\]
\[
\lim_{(h,k) \to (0,0)} |f_x(x_1, y+k) - f_x(x,y)| = 0
\]
E
\[
\lim_{(h,k) \to (0,0)} f_y(x, y_1) = f_y(x,y)
\]
\[
\lim_{(h,k) \to (0,0)} |f_y(x, y_1) - f_y(x,y)| = 0
\]
Essendo dei valori assoluti tendono a zero quando $(h,k) \to (0,0)$, quindi si può affermare che il limite è nullo, allora la funzione è differenziabile.

\hfill{\color{thmcolor}$\square$}

\textbf{93. Come si può definire una gerarchia di regolarità {\color{red}(funzioni di classe $C^k$)}?}

Sia $f: A \subseteq \mathbb{R}^n \rightarrow\mathbb{R}$ con $A$ aperto, definiamo $C^1(A)$ come l'insieme delle funzioni con derivate parziali continue su $A$. 

Vale la seguente catena di implicazioni per funzioni di più variabili:
\[
    f \in C^1(A) \implies f \text{ differenziabile in } A \implies f \in C^0(A) \text{ continua in ogni punto di }A
\]

$f \in C^1$ è una condizione \textit{sufficiente} ma non necessaria per la differenziabilità. Esistono funzioni differenziabili che hanno derivate parziali discontinue (casi rari e patologici).\\

\textbf{94. Se la funzione è {\color{red}differenziabile} in un punto, quindi continua, {\color{red}ammette tutte le derivate direzionali} nel punto?}

Sia $f: A \to \mathbb{R}$, con $A \subset \mathbb{R}^n$ aperto e $x_0 \in A$.
Se $f$ è differenziabile in $\mathbf{x}_0$ con differenziale $L$, allora:
\begin{enumerate}
    \item $f$ è continua in $\mathbf{x}_0$;
    \item $f$ ammette tutte le derivate direzionali in $\mathbf{x}_0$ e
    \[
    D_{\vec{v}} f(\mathbf{x}_0) = L(\vec{v}), \quad \forall \vec{v} \in \mathbb{R}^n.
    \]
\end{enumerate}

\textbf{\color{thmcolor}DIMOSTRAZIONE}

In particolare, si può dimostrare che il differenziale $L$, se esiste, è \textbf{unico}.
Sia $\mathbf{v} \in \mathbb{R}^n$. Per definizione, la derivata direzionale è:
\[
D_{\mathbf{v}} f(\mathbf{x}_0) = \lim_{t \to 0} \frac{f(\mathbf{x}_0 + t\mathbf{v}) - f(\mathbf{x}_0)}{t}
\]
Usando la definizione di differenziabilità con incremento $\mathbf{h} = t\mathbf{v}$, si ottiene:
\[
f(\mathbf{x}_0 + t\mathbf{v}) = f(\mathbf{x}_0) + L(t\mathbf{v}) + o(\|t\mathbf{v}\|)
\]
Per la linearità di $L$, sappiamo che $L(t\mathbf{v}) = tL(\mathbf{v})$. Sostituendo e portando $f(\mathbf{x}_0)$ a sinistra:
\[
f(\mathbf{x}_0 + t\mathbf{v}) - f(\mathbf{x}_0) = tL(\mathbf{v}) + o(|t|\|\mathbf{v}\|)
\]
Dividendo tutto per $t \neq 0$:
\[
\frac{f(\mathbf{x}_0 + t\mathbf{v}) - f(\mathbf{x}_0)}{t} = L(\mathbf{v}) + \frac{o(|t|\|\mathbf{v}\|)}{t}
\]
Passando al limite per $t \to 0$, il termine con l'o-piccolo tende a 0. Segue che:
\[
D_{\mathbf{v}} f(\mathbf{x}_0) = L(\mathbf{v})
\]
Poiché il limite (la derivata direzionale) è unico, anche l'applicazione lineare $L$ deve essere unica.

\hfill{\color{thmcolor}$\square$}

\textbf{95. Perché diciamo che il vettore {\color{red}gradiente} indica la direzione e il verso della {\color{red}massima pendenza} di una funzione multivariata in un punto?}

Per comprendere questo fatto geometrico, usiamo la \textbf{regola della catena}.
Consideriamo una curva regolare che passa per il punto $\mathbf{x}_0$:
\[
\gamma: [-1,1] \subset \mathbb{R} \to A \subseteq \mathbb{R}^n
\]
Tale che $\gamma(0)=\mathbf{x}_0$ e $\gamma'(0)=\mathbf{v}$ (dove $\mathbf{v}$ è il vettore direzione unitario, $\|\mathbf{v}\|=1$).

Definiamo la funzione composta (restrizione di $f$ alla curva):
\[
F(t) = (f \circ \gamma)(t) = f(\gamma(t))
\]
Questa funzione rappresenta la "quota" percorrendo la curva $\gamma$.
Poiché $f$ è differenziabile, la derivata di $F$ in $t=0$ (la variazione di quota istantanea) è data dalla regola della catena:
\[
F'(0) = \nabla f(\mathbf{x}_0) \cdot \gamma'(0) = \nabla f(\mathbf{x}_0) \cdot \mathbf{v}
\]

Informalmente:
\begin{itemize}
    \item $\mathbf{x}_0$ è il punto dove ci si trova sulla superficie;
    \item la curva $\gamma(t)$ è il "sentiero" che si decide di percorrere;
    \item $\gamma'(0)=\mathbf{v}$ è il vettore che rappresenta verso dove ci stiamo muovendo;
    \item $F(t) = f(\gamma(t))$ è la quota (altitudine) a cui si cammina.
\end{itemize}
La derivata $F'(0)$ misura quanto velocemente cambia la quota nel tempo, ossia la pendenza sul "sentiero". La regola della catena ci dice praticamente che questa pendenza dipende dal prodotto scalare $\nabla f(\mathbf{x}_0) \cdot \mathbf{v}$.\\

Ora, immaginando di scegliere una direzione qualsiasi $\mathbf{v}$ in cui muoverci ma con il vincolo che la velocità sia unitaria ($\|\mathbf{v}\|=1$), possiamo comprendere quale direzione ci fa salire più velocemente. 

Secondo la proposizione precedente, la derivata direzionale $D_{\mathbf{v}} f$ è data da:
\[
 D_{\mathbf{v}} f(\mathbf{x}_0) = \nabla f(\mathbf{x}_0) \cdot \mathbf{v}
\]

Ricordando la definizione geometrica di prodotto scalare ($\mathbf{a} \cdot \mathbf{b} = \|\mathbf{a}\| \|\mathbf{b}\| \cos \theta$), possiamo scrivere:
\[
 D_{\mathbf{v}} f = \|\nabla f(\mathbf{x}_0)\| \cdot \underbrace{\|\mathbf{v}\|}_{1} \cdot \cos \theta = \|\nabla f(\mathbf{x}_0)\| \cos \theta
\]
dove $\theta$ è l'angolo tra il gradiente e la direzione scelta $\mathbf{v}$.

Per massimizzare la pendenza $D_{\mathbf{v}} f$, dobbiamo massimizzare $\cos \theta$.
Il massimo si ha per $\theta = 0$, ovvero quando la direzione $\mathbf{v}$ è \textbf{parallela e concorde} al gradiente $\nabla f(\mathbf{x}_0)$.

Analizzando il termine $\cos \theta$:
\begin{itemize}
    \item \textbf{Massima Crescita ($\theta = 0$):} La derivata è massima quando ci muoviamo nella direzione del gradiente.
        \[ D_{\mathbf{v}} f = \|\nabla f\| \]
    \item \textbf{Massima Decrescita ($\theta = \pi$):} La derivata è minima (negativa) quando ci muoviamo in direzione opposta al gradiente.
        \[ D_{\mathbf{v}} f = -\|\nabla f\| \]
    \item \textbf{Crescita Nulla ($\theta = \pi/2$):} La funzione non cambia valore se ci muoviamo ortogonalmente al gradiente (lungo le curve di livello).
\end{itemize}

\begin{figure}[H] 
    \centering
    \includegraphics[width=0.6\linewidth]{../Immagini/ProdottoScalare.jpg}
\end{figure}
\textbf{96. Cosa si intende per {\color{red}formula del gradiente?}}

Sia $f$ differenziabile su $A$ e sia un vettore $\vec{v}$ normato, ossia $\|\vec{v}\|= \sqrt{v_1^2+v_2^2}$, si può definire la derivata direzionale con la formula
\[
D_{\vec{v}}f(x,y) = \langle f(x,y), \vec{v} \rangle
\]

\textbf{97. Enunciare e dimsotrare la regola della derivazione composta o {\color{red}regola della catena}.}

Se $f$ è differenziabile e $\gamma$ è derivabile, allora la derivata della funzione composta $F(t) = f(\gamma(t))$ è:
\[
    F'(t) = \langle \nabla f(\gamma(t)), \gamma'(t) \rangle
\]
Che equivale al prodotto scalare tra il gradiente di $f$ e il vettore tangente alla curva $\gamma$.\\

\textbf{\color{thmcolor}DIMOSTRAZIONE}

Consideriamo il rapporto incrementale di $F(t)$ rispetto a un incremento $h \in \mathbb{R}$:
\[
    \frac{F(t+h) - F(t)}{h} = \frac{f(\gamma(t+h)) - f(\gamma(t))}{h}
\]
Poiché per ipotesi $f$ è differenziabile nel punto $\mathbf{x} = \gamma(t)$, possiamo scrivere l'incremento di $f$ come somma della parte lineare (gradiente) e dell'errore (o-piccolo).
Allora:
\[
    f(\gamma(t+h)) - f(\gamma(t)) = \langle \nabla f(\gamma(t)), \gamma(t+h) - \gamma(t) \rangle + o(\|\gamma(t+h) - \gamma(t)\|)
\]
Sostituiamo questa espressione nel rapporto incrementale:
\[
    \frac{F(t+h) - F(t)}{h} = \frac{\langle \nabla f(\gamma(t)), \gamma(t+h) - \gamma(t) \rangle}{h} + \frac{o(\|\gamma(t+h) - \gamma(t)\|)}{h}
\]
Per la linearità del prodotto scalare, possiamo portare $1/h$ dentro il primo termine:
\[
    = \left\langle \nabla f(\gamma(t)), \frac{\gamma(t+h) - \gamma(t)}{h} \right\rangle + \frac{o(\|\gamma(t+h) - \gamma(t)\|)}{h}
\]
Passiamo ora al limite per $h \to 0$:


Poiché $\gamma$ è derivabile in $t$, il rapporto incrementale vettoriale tende alla derivata $\gamma'(t)$:
\[
    \lim_{h \to 0} \frac{\gamma(t+h) - \gamma(t)}{h} = \gamma'(t)
\]
Quindi il primo termine tende a:
\[
    \langle \nabla f(\gamma(t)), \gamma'(t) \rangle
\]
Che è esattamente la tesi che cerchiamo. Dobbiamo ora dimostrare che il resto tende a 0.

Dobbiamo mostrare che:
\[
    \lim_{h \to 0} \frac{o(\|\gamma(t+h) - \gamma(t)\|)}{h} = 0
\]
Supponendo sia non nullo, riscriviamo
\[
\Bigg| \frac{o(\|\gamma(t+h) - \gamma(t)\|)}{h} \Bigg|
\]
Moltiplichiamo e dividiamo per la norma dell'incremento $\|\gamma(t+h) - \gamma(t)\|$ :
\[
    \lim_{h \to 0} \underbrace{\frac{o(\|\gamma(t+h) - \gamma(t)\|)}{\|\gamma(t+h) - \gamma(t)\|}}_{\text{(A)}} \cdot \underbrace{\frac{\|\gamma(t+h) - \gamma(t)\|}{|h|}}_{\text{(B)}} 
\]
Analizziamo i fattori:
\begin{itemize}
    \item \textbf{Fattore (A):} Tende a 0 per definizione di o-piccolo. Infatti, per la continuità di $\gamma$, quando $h \to 0$ anche l'incremento $\|\gamma(t+h) - \gamma(t)\| \to 0$.
    \item \textbf{Fattore (B):}.
    \[
    \lim_{h \to 0} \Biggl[ \sum_{i=1}^n\Biggl( \frac{ \gamma_i(t+h)-\gamma_i(t)}{h}\Biggr)^2\Biggr]^{\frac{1}{2}} = \Bigg[ \sum_{i=1}^n (\gamma'(t))^2\Bigg]^{\frac{1}{2}} \in \mathbb{R}
    \]
    Tende alla norma della derivata $\|\gamma'(t)\|$, che è un numero reale finito (la derivata esiste per ipotesi)

    
\end{itemize}
Quindi abbiamo un termine che tende a $0$ moltiplicato per una quantità limitata ($\|\gamma'(t)\|$). Il limite complessivo è $0$.

Ricomponendo i pezzi, il limite del rapporto incrementale è:
\[
    F'(t) = \langle \nabla f(\gamma(t)), \gamma'(t) \rangle + 0
\]

\hfill{\color{thmcolor}$\square$}

\section{Derivate successive}
\textbf{98. Cosa sono {\color{red}le derivate successive} di una funzione multivariata?}

Sia $f:A \subseteq \mathbb{R}^n \to \mathbb{R}$ funzione derivabile su $A$, ossia esistono e sono ben definite le derivate
\[ 
\frac{\partial f}{\partial x_1}(\mathbf{x}) \dots \frac{\partial f}{\partial x_n}(\mathbf{x})
\]
Si suppone che $\forall f_{x_i}(\mathbf{x}): A \to \mathbb{R}$ per qualche $i= 1 \dots n$ accade che la funzione $f_{x_i(\mathbf{x})}$ sia derivabile ulteriormente, allora esiste la derivata successiva di $f$ in $\mathbf{x}$
\[
\frac{\partial}{\partial x_j} \Bigg(\frac{\partial f}{\partial x_i}(\mathbf{x}) \Bigg) = f_{x_i x_j}(\mathbf{x}) = \frac{\partial^2 f}{\partial x_i \partial x_j}(\mathbf{x})
\]
La derivata seconda è, praticamente, la derivata parziale delle derivate prime di f e indicano la curvatura sulla superficie della funzione.\\

\subsection{Matrice Hessiana}
\textbf{99. Qual è l'equivalente del gradiente per {\color{red}le derivate seconde di una funzione}?}

Per raccogliere tutte le informazioni sulla curvatura della funzione, ci rifacciamo all'algebra lineare dove per ottenere uno scalare (un numero) moltiplicando dei vettori "due volte" (termini al quadrato tipo $x^2$ o misti $xy$), l'oggetto centrale deve essere necessariamente una matrice quadrata.\\

Nel caso generale, le derivate seconde di una funzione $f$ sono date dal variare degli indici $i$ e $j$ tra $1$ e $n$.
Quindi saranno $n \times n$ derivate seconde possibili organizzate in una \underline{Matrice Hessiana}: 

\[
H_f(\mathbf{x}) = 
\begin{bmatrix}
\frac{\partial^2 f}{\partial x_1^2} & \frac{\partial^2 f}{\partial x_1 \partial x_2} & \cdots & \frac{\partial^2 f}{\partial x_1 \partial x_n} \\
\frac{\partial^2 f}{\partial x_2 \partial x_1} & \frac{\partial^2 f}{\partial x_2^2} & \cdots & \frac{\partial^2 f}{\partial x_2 \partial x_n} \\
\vdots & \vdots & \ddots & \vdots \\
\frac{\partial^2 f}{\partial x_n \partial x_1} & \frac{\partial^2 f}{\partial x_n \partial x_2} & \cdots & \frac{\partial^2 f}{\partial x_n^2}
\end{bmatrix}
\]

\vspace{1cm}

In forma compatta si scrive anche
\[
Hf(\mathbf{x}) = 
\begin{bmatrix}
f_{x_1 x_1} & f_{x_1 x_2} & \cdots & f_{x_1 x_n} \\
f_{x_2 x_1} & f_{x_2 x_2} & \cdots & f_{x_2 x_n} \\
\vdots & \vdots & \ddots & \vdots \\
f_{x_n x_1} & f_{x_n x_2} & \cdots & f_{x_n x_n}
\end{bmatrix}
\]
Se $i=j$ la derivata si dice pura e si scrive
\[
f_{x_i x_i}(\mathbf{x}) = \frac{\partial^2 f}{\partial x^2_i}(\mathbf{x})
\]
Se, invece, $i \not = j$ la derivata si dice mista e si scrive
\[
f_{x_i x_j}(\mathbf{x}) = \frac{\partial^2 f}{\partial x_i x_j}(\mathbf{x})
\]

\textbf{100. Come si definisce {\color{red}una matrice hessiana nel caso di $n=2$}?}

Sia $f: A \subseteq \mathbb{R}^n \to \mathbb{R}$ e supponiamo che $f$ sia derivabile $2$ volte nell'insieme $A$, ossia $\forall (x,y) \in A$ esistono tutte e $4$ le derivate seconde: 
\[
f_{xx}(x,y), f_{xy}(x,y), f_{yx}(x,y), f_{yy}(x,y)
\]
Queste derivate seconde sono organizzate in una matrice hessiana di dimensione $2 \times 2$
\[
Hf(x,y) = 
\begin{bmatrix}
\frac{\partial^2 f}{\partial x^2}(x,y) & \frac{\partial^2 f}{\partial x \partial y}(x,y)\\
\frac{\partial^2 f}{\partial y \partial x}(x,y) & \frac{\partial^2 f}{\partial y^2}(x,y) 
\end{bmatrix}
\]
E' fondamentale affermare che, nel caso generale, \textbf{non} possiamo dire che la matrice hessiana sia simmetrica, solo dopo le opportune analisi dimostrando che appartiene alla classe $C^2$ potremo verificarlo.\\

\textbf{101. Enunciare e dimostrare {\color{red}il teorema di Schwarz}.}
\begin{teorema}[Teorema di Schwarz] Sia $f: A \subseteq \mathbb{R}^2 \to \mathbb{R}$ una funzione definita su un aperto $A$. Sia $P_0 = (x_0, y_0) \in A$ e supponiamo che la funzione sia derivabile due volte su $A$, ossia che esistano $n \times n$ derivate seconde della forma $D_{x_i x_j}(\mathbf{x})$ $\forall i,j=1 \dots n$ e $\forall \mathbf{x} \in A$.
Se le derivate miste $f_{xy}$ e $f_{yx}$ esistono in un intorno di $P_0$ e sono continue in $P_0$, allora coincidono:
\[
f_{xy}(x_0, y_0) = f_{yx}(x_0, y_0)
\]
Ovvero:
\[
\frac{\partial^2 f}{\partial x \partial y}(x_0, y_0) = \frac{\partial^2 f}{\partial y \partial x}(x_0, y_0)
\]
\end{teorema} 

\textbf{\color{thmcolor}DIMOSTRAZIONE} su $n=2$

Sia $P = (x, y) \in A$ con $P \neq P_0$. Si definiscono le seguenti funzioni a partire da $f$:
\[ F(x) = f(x, y) - f(x, y_0) \quad (\text{con } \forall y \text{ fissato}) \]
\[ G(y) = f(x, y) - f(x_0, y) \quad (\text{con } \forall x \text{ fissato}) \]

\textbf{{\color{thmcolor}Passo 1}}: Applicando il teorema di Lagrange a $F(x)$ nell'intervallo di estremi $x_0$ e $x$, esiste un punto $x_1 \in (x_0, x)$ tale che:
\[ F(x) - F(x_0) = F'(x_1)(x - x_0) \]
Poiché la derivata di $F$ rispetto a $x$ è data dalla differenza delle derivate parziali di $f$:
\[ F'(x) = \frac{\partial f}{\partial x}(x, y) - \frac{\partial f}{\partial x}(x, y_0) \]
L'uguaglianza diventa:
\[ F(x) - F(x_0) = \left( \frac{\partial f}{\partial x}(x_1, y) - \frac{\partial f}{\partial x}(x_1, y_0) \right)(x - x_0) \]

Applichiamo nuovamente Lagrange alla funzione $\frac{\partial f}{\partial x}$ (vista come funzione di $y$) nell'intervallo $(y_0, y)$. Esiste un punto $y_1 \in (y_0, y)$ tale che:
\[ \frac{\partial f}{\partial x}(x_1, y) - \frac{\partial f}{\partial x}(x_1, y_0) = \frac{\partial^2 f}{\partial y \partial x}(x_1, y_1)(y - y_0) \]
Sostituendo, otteniamo l'espressione finale per la differenza di $F$:
\[ F(x) - F(x_0) = \frac{\partial^2 f}{\partial y \partial x}(x_1, y_1)(y - y_0)(x - x_0) \]

Analogamente, si applica lo stesso ragionamento alla funzione $G(y)$ nell'intervallo di estremi $y_0$ e $y$. Esiste un punto $y_2 \in (y_0, y)$ tale che:
\[ G(y) - G(y_0) = G'(y_2)(y - y_0) = \left( \frac{\partial f}{\partial y}(x, y_2) - \frac{\partial f}{\partial y}(x_0, y_2) \right)(y - y_0) \]
Applicando Lagrange rispetto a $x$ alla funzione $\frac{\partial f}{\partial y}$ nell'intervallo $(x_0, x)$, esiste un punto $x_2 \in (x_0, x)$ tale che:
\[ G(y) - G(y_0) = \frac{\partial^2 f}{\partial x \partial y}(x_2, y_2)(x - x_0)(y - y_0) \]

\textbf{{\color{thmcolor}Passo 2}}: Osserviamo che le due differenze iniziali sono algebricamente identiche
\[ F(x) - F(x_0) = [f(x, y) - f(x, y_0)] - [f(x_0, y) - f(x_0, y_0)] \]
\[ G(y) - G(y_0) = [f(x, y) - f(x_0, y)] - [f(x, y_0) - f(x_0, y_0)] \]
Quindi $F(x) - F(x_0) = G(y) - G(y_0)$. Uguagliando le espressioni ottenute e dividendo per $(x - x_0)(y - y_0) \neq 0$:
\[ \frac{\partial^2 f}{\partial y \partial x}(x_1, y_1) = \frac{\partial^2 f}{\partial x \partial y}(x_2, y_2) \]

\textbf{{\color{thmcolor}Passo 3}}: Per l'ipotesi di continuità delle derivate miste in $P_0$, passando al limite per $(x, y) \to (x_0, y_0)$, i punti intermedi $(x_1, y_1)$ e $(x_2, y_2)$ tendono entrambi a $(x_0, y_0)$:
\[ \lim_{(x,y) \to (x_0, y_0)} \frac{\partial^2 f}{\partial y \partial x}(x_1, y_1) = \frac{\partial^2 f}{\partial y \partial x}(x_0, y_0) \]
\[ \lim_{(x,y) \to (x_0, y_0)} \frac{\partial^2 f}{\partial x \partial y}(x_2, y_2) = \frac{\partial^2 f}{\partial x \partial y}(x_0, y_0) \]

L'uguaglianza dei limiti dimostra che le derivate miste sono uguali e l'ordine di derivazione non importa:
\[ \frac{\partial^2 f}{\partial y \partial x}(x_0, y_0) = \frac{\partial^2 f}{\partial x \partial y}(x_0, y_0) \]


Possiamo affermare che  se $f \in C^2(A)$, la matrice Hessiana è simmetrica in ogni punto di $A$.

\hfill{\color{thmcolor}$\square$}

La sola esistenza delle derivate miste in un punto non garantisce l'uguaglianza. 
È fondamentale l'ipotesi di \textbf{continuità} delle derivate seconde.
Esistono controesempi (es. la funzione di Peano-Schwarz $f(x,y) = \frac{xy(x^2-y^2)}{x^2+y^2}$) dove le derivate miste nell'origine esistono ma sono diverse ($f_{xy} \neq f_{yx}$) proprio perché non sono continue in $(0,0)$.\\

\section{La formula di Taylor in funzioni multivariate}

\textbf{102. Come si può generalizzare {\color{red}la formula di Taylor a più dimensioni}?}

Dire che \( f \in C^2(A) \) significa che la matrice hessiana \( Hf(A) \) è simmetrica in ogni punto dell’aperto \( A \): 
\[
Hf(\mathbf{x}) = \begin{pmatrix}
f_{xx} & f_{xy} \\
f_{yx} & f_{yy}
\end{pmatrix}
\quad \text{è una matrice simmetrica.}
\]
Questo implica che $Hf = Hf^T$.

In generale, si dice che \( f \in C^k(A) \), con \( k \in \mathbb{N} \), se tutte le derivate parziali fino all’ordine \( k \) esistono e sono continue.\\

L'\textit{idea fondamentale} si basa sull'analizzare il comportamento di una funzione $f: A \subseteq \mathbb{R}^n \to \mathbb{R}$ che si suppone sia di classe $C^2(A)$ lungo un segmento rettilineo.

Siano \( \mathbf{x}, \mathbf{x}+\mathbf{h} \in A \subseteq \mathbb{R}^n \) con \( \mathbf{h} \neq 0 \).

Poiché $A$ è aperto, per $\mathbf{h}$ abbastanza piccolo possiamo dire che il segmento di estremi \( \mathbf{x} \) e \( \mathbf{x}+\mathbf{h} \) è contenuto in $A$:
\[
[\mathbf{x}, \mathbf{x}+\mathbf{h}] = \{ \mathbf{x}(t) \in \mathbb{R}^n : \mathbf{x}(t) = \mathbf{x} + t \mathbf{h},\; t \in [0,1] \} \subset A.
\]

Si osserva che la parametrizzazione soddisfa le condizioni agli estremi:
\[
\mathbf{x}(0) = \mathbf{x}, \qquad \mathbf{x}(1) = \mathbf{x} + \mathbf{h}.
\]

Considerando una funzione composta (applicando la regola di derivazione delle funzioni composte) possiamo scrivere la funzione ausiliaria (restrizione di $f$ al segmento), sia $F : [0,1] \to \mathbb{R}$:
\[
F(t) = f(\mathbf{x} + t \mathbf{h}), \quad \forall t \in [0,1].
\]
Il nostro obiettivo, adesso, è quello di dimostrare che dalla formula di Taylor centrata in $t$ possiamo ricavare informazioni su $f(\mathbf{x})$.\\

Se \( f \in C^1(A) \), allora \( F \in C^1([0,1]) \).
La derivata prima \( F'(t) \) conla regola della catena:
\[
F'(t) = (f(\mathbf{x} + t \mathbf{h}))' = \langle \nabla f(\mathbf{x} + t \mathbf{h}), x'(t) \rangle.
\]

Poiché \( x'(t) = \mathbf{h} \), si ha:
\[
F'(t) = \langle \nabla f(\mathbf{x} + t \mathbf{h}), \mathbf{h} \rangle
= \sum_{i=1}^{n} \frac{\partial f}{\partial x_i}(\mathbf{x} + t \mathbf{h})\, h_i.
\]

Poiché abbiamo supposto che $f \in C^2(A)$, la funzione $F(t)$ è derivabile due volte. Per calcolare $F''(t)$ dobbiamo derivare nuovamente l'espressione di $F'(t)$ rispetto alla variabile $t$. Applicando nuovamente la regola della catena a ciascun termine di $\frac{\partial f}{\partial x_i}$ otteniamo:
\[
F''(t) =  \sum_{i,j = 1}^n \frac{\partial f}{\partial x_j} \Big (\frac{\partial f}{\partial x_i}(\mathbf{x}+t \mathbf{h}) \cdot h_i \Big ) \cdot h_j = \sum_{i,j=1}^{n} \frac{\partial^2 f}{\partial x_i,\partial x_j}(\mathbf{x} + t \mathbf{h}) \cdot h_i \cdot h_j
\]
\textit{Formalmente}, è un polinomio omogeneo di secondo grado con le componenti del vettore $\mathbf{h}$ che appaiono moltiplicate fra di loro. I coefficenti di questo polinomio sono proprio le derivate seconde della funzione in ($\mathbf{x},\mathbf{x}+t\mathbf{h}$):
\begin{itemize}
    \item $F(0)=f(\mathbf{x}(0))= f(\mathbf{x}) = \mathbf{x}$
    \item $F(1)=f(\mathbf{x}(1))= f(\mathbf{x}+\mathbf{h}) = \mathbf{x}+\mathbf{h}$
\end{itemize}
Allora applicando la formula di Taylor alla $F(t)$ centrata nel punto $t=0$ ottengo
\[
F(t)= P_n(t,0) + R_n(t,0)
\]
Generalizzando, se la funzione $f \in C^k(A)$ allora trasferisce la sua regolarità su una composta $F$. Possiamo affermare che $\exists \theta \in \mathbb{R}$ che sta tra $[0,1]$ con resto tale che 
\[
F(1)=F(0)+F'(0)+\frac{F''(0)}{2!}+ \dots+\frac{F^{k-1}(0)}{k-1}+\underbrace{\frac{F^k(\theta)}{k!}}_\text{$R_n$}
\]

\textit{Informalmente}, invece di studiare il comportamento della funzione $f$ nell'intero dominio multidimensionale $A\subseteq\mathbb{R}$, stiamo "affettando" il grafico della funzione lungo una retta. 
Definendo $F(t)=f(\mathbf{x}+t\mathbf{x})$, stiamo restringendo il nostro campo d'osservazione a ciò che accade esclusivamente lungo il segmento $(x,x+h)$.

I punti chiave di questa osservazione sono:
\begin{itemize}
    \item \textbf{La Funzione come Binario}
    
    La variabile scalare t agisce come un parametro temporale che ci fa scorrere lungo un binario rettilineo. Questo ci permette di ricondurre lo studio di una funzione di n variabili allo studio di una funzione di una sola variabile reale ($F:[ 0,1] \to \mathbb{R}$);

    \item \textbf{La Derivata Direzionale}

    Il risultato $F'(t) = \langle \nabla f(\mathbf{x} + t \mathbf{h}), \mathbf{h} \rangle$ non è altro che la derivata direzionale di $f$ lungo il vettore $\mathbf{h}$ (a meno di un fattore di normalizzazione). Ci dice quanto velocemente cambia il valore della funzione mentre ci muoviamo in quella specifica direzione $\mathbf{h}$, partendo dal punto $\mathbf{x}+t\mathbf{h}$.

    \item \textbf{Ponte verso i Teoremi di Taylor e del Valor Medio}

    Questa costruzione è il ponte logico necessario per estendere i teoremi del calcolo monodimensionale a quello multidimensionale. Applicando il Teorema del Valor Medio o lo sviluppo di Taylor alla funzione ausiliaria $F(t)$ nell'intervallo $[0,1]$, possiamo dimostrare rigorosamente le proprietà di $f $ in $ \mathbb{R}^n$, come l'esistenza di massimi/minimi o l'approssimazione quadratica tramite la matrice Hessiana.
\end{itemize}
\newpage
\textbf{103. Enunciare e dimostrare {\color{red}la formula di Taylor secondo il resto secondo Lagrange}.}
La formula di Taylor rispetto a $F'$ con resto secondo Lagrange equivale al teorema di Lagrange per funzioni in più variabili.
\begin{teorema}Sia $f:A \subseteq \mathbb{R}^n \to \mathbb{R}$ con $A$ un aperto e $f \in C^k(A)$.

Ipotizziamo che  $\mathbf{x} $ e $ ( \mathbf{x}+\mathbf{h} )$ ($\mathbf{h}\not=0$) siano due elementi di $A$ per cui il segmento di estremi $ [\mathbf{x}, (\mathbf{x}+\mathbf{h})]\ \subset A$ Si può trovare un $\theta$ reale $\in [0,1]$ tale che\\

Nel caso in cui $k=1$:
\[
f(\mathbf{x}+\mathbf{h}) = f(\mathbf{x})+ \sum_{i=1}^n \frac{\partial f}{\partial x_i}(\mathbf{x}+\theta \mathbf{h}) h_i= f(\mathbf{x}) +\langle \nabla f(\mathbf{x}+\theta \mathbf{h}), \mathbf{h} \rangle
\]
che equivale alla formula di Taylor $T_n \Rightarrow F(1) = F(0) +F'(0)$\\

Nel caso in cui $k=2$:
\[
f(\mathbf{x}+\mathbf{h}) = f(\mathbf{x}) +\langle \nabla f(\mathbf{x}+\theta \mathbf{h}), \mathbf{h} \rangle + \frac{1}{2} \sum_{i,j=1}^n \frac{\partial^2 f}{\partial x_i \partial x_j}(\mathbf{x}+\theta \mathbf{h})h_i h_j
\]
che in forma compatta viene $f(\mathbf{x})+\langle \nabla f(\mathbf{x}), \mathbf{h}\rangle +  \frac{1}{2} \langle He f(\mathbf{x}+\theta \mathbf{h}), \mathbf{h} \rangle$
\end{teorema}

\textbf{\color{thmcolor}DIMOSTRAZIONE} del caso k = 2

La matrice Hessiana in ogni punto di $A$ è una matrice $n \times n$:
\[
Hf(\mathbf{x}) \cdot \mathbf{h} \quad \text{dove } \mathbf{h} \text{ è un vettore } (n \times 1)
\]

In termini matriciali:
\[
\underbrace{\left[ \begin{matrix} \dots \\ \dots \end{matrix} \right]}_{n \times n} \cdot \underbrace{\left[ \begin{matrix} \vdots \\ \vdots \end{matrix} \right]}_{n \times 1} \to \text{Vettore colonna}
\]

Esplicitamente la matrice Hessiana è:
\[
Hf(\mathbf{x} + \theta \mathbf{h}) = 
\begin{bmatrix}
\frac{\partial^2 f}{\partial x_1^2}(\mathbf{x}+\theta \mathbf{h}) & \dots & \frac{\partial^2 f}{\partial x_1 \partial x_n}(\mathbf{x}+\theta \mathbf{h}) \\
\vdots & \ddots & \vdots \\
\frac{\partial^2 f}{\partial x_n \partial x_1}(\mathbf{x}+\theta \mathbf{h}) & \dots & \frac{\partial^2 f}{\partial x_n^2}(\mathbf{x}+\theta \mathbf{h})
\end{bmatrix}
\]
\hfill{\color{thmcolor}$\square$}

\textbf{103. Enunciare e dimostrare {\color{red}la formula di Taylor secondo il resto secondo Peano}.}

Sia $f: A \subseteq \mathbb{R}^n \to \mathbb{R}$ e $f \in C^2(A)$. 
Siano $\mathbf{x}$ e $\mathbf{x}+\mathbf{h}$ (distinti, per $\mathbf{h} \neq \mathbf{0}$) appartenenti ad $A$, per cui il segmento con estremi $[\mathbf{x}, \mathbf{x}+\mathbf{h}] \subseteq A$.
Allora:
\[
f(\mathbf{x}+\mathbf{h}) = f(\mathbf{x}) + \langle \nabla f(\mathbf{x}), \mathbf{h} \rangle + \frac{1}{2} \langle Hf(\mathbf{x})\mathbf{h}, \mathbf{h} \rangle + o(\|h\|^2) \quad \text{per } \mathbf{h} \to \mathbf{0}
\]

\textbf{\color{thmcolor}DIMOSTRAZIONE} del caso n = 2

Sia $f: A \subseteq \mathbb{R}^2 \to \mathbb{R}$. Consideriamo i 2 punti $(\mathbf{x}_0, \mathbf{y}_0)$ e l'incremento $\mathbf{h}$.
\[
\mathbf{x} = (x_0, y_0), \quad \mathbf{h} = (h, k) \in \mathbb{R}^2
\]
Quindi il punto incrementato è:
\[
\mathbf{x} + \mathbf{h} = (x_0+h, y_0+k)
\]

La formula si scrive:
\[
f(x_0+h, y_0+k) = f(x_0, y_0) + \frac{\partial f}{\partial x}(x_0,y_0)h + \frac{\partial f}{\partial y}(x_0,y_0)k + \frac{1}{2} \langle Hf(x_0,y_0)\mathbf{h}, \mathbf{h} \rangle
\]

Analizziamo il termine quadratico (Hessiano). Poiché $f \in C^2(A)$, per il teorema di Schwarz la matrice è simmetrica ($f_{xy} = f_{yx}$):
\[
Hf(x_0, y_0) = 
\begin{bmatrix}
\frac{\partial^2 f}{\partial x^2}(x_0,y_0) & \frac{\partial^2 f}{\partial x \partial y}(x_0,y_0) \\
\frac{\partial^2 f}{\partial y \partial x}(x_0,y_0) & \frac{\partial^2 f}{\partial y^2}(x_0,y_0)
\end{bmatrix}
\]

Questo va moltiplicato per $\mathbf{h} = \binom{h}{k}$.
Prima moltiplicazione (Matrice $\times$ Vettore colonna):
\[
\begin{bmatrix}
f_{xx} & f_{xy} \\
f_{yx} & f_{yy}
\end{bmatrix}
\begin{bmatrix} h \\ k \end{bmatrix}
=
\begin{bmatrix}
\frac{\partial^2 f}{\partial x^2}h + \frac{\partial^2 f}{\partial x \partial y}k \\
\frac{\partial^2 f}{\partial y \partial x}h + \frac{\partial^2 f}{\partial y^2}k
\end{bmatrix}
\]

Tutto questo va poi moltiplicato per $\mathbf{h}^T$ (vettore riga) a sinistra:
\[
\begin{bmatrix} h & k \end{bmatrix} \cdot 
\begin{bmatrix}
f_{xx}h + f_{xy}k \\
f_{yx}h + f_{yy}k
\end{bmatrix}
\]

Svolgendo il prodotto:
\[
\frac{1}{2} \left[ \frac{\partial^2 f}{\partial x^2}(x_0,y_0)h^2 + \frac{\partial^2 f}{\partial x \partial y}(x_0,y_0)hk + \frac{\partial^2 f}{\partial y \partial x}(x_0,y_0)hk + \frac{\partial^2 f}{\partial y^2}(x_0,y_0)k^2 \right]
\]

Grazie alla simmetria ($f_{xy} = f_{yx}$), i termini centrali si sommano:
\[
\text{Espressione omogenea di 2° grado in } h,k: \quad \frac{\partial^2 f}{\partial x^2}h^2 + 2\frac{\partial^2 f}{\partial x \partial y}hk + \frac{\partial^2 f}{\partial y^2}k^2
\]

Tutto sommato per l'errore:
\[
+ o(\sqrt{h^2+k^2}^2) = o(h^2+k^2) \quad \text{con } h,k \to 0
\]

Riscrivendo in modo opportuno ponendo $x = x_0+h \implies h = x-x_0$ e $y = y_0+k \implies k = y-y_0$:

\[
P_2(x,y) = f(x_0,y_0) + \frac{\partial f}{\partial x}(x-x_0) + \frac{\partial f}{\partial y}(y-y_0) + \frac{1}{2} \left[ f_{xx}(x-x_0)^2 + 2f_{xy}(x-x_0)(y-y_0) + f_{yy}(y-y_0)^2 \right]
\]
\hfill{\color{thmcolor}$\square$}

Questo è l'unico polinomio in $x,y$ di grado $\le 2$ per cui si ha che la differenza tra $f(x,y)$ e $P_2$ va a zero più rapidamente della distanza al quadrato per i 2 punti $o \|((x-x_0)^2 + (y-y_0)^2)\|$.
\[
\lim_{(x,y) \to (x_0,y_0)} \frac{f(x,y) - P_2(x,y)}{\|(x,y)-(x_0,y_0)\|^2} = 0
\]

Nel caso $n$ variabili (caso generale), il termine $\langle Hf(x_0)h, h \rangle$ "esce fuori" un'espressione omogenea di 2° grado in $h_1, \dots, h_n$, dove $n$ è il numero di variabili, e si indicherà come $q(h)$, i cui coefficienti sono le opportune derivate seconde.

\section{Studio dei massimi e minimi di funzioni multivariate}
\textbf{104. Come si definiscono {\color{red}massimi e minimi locali} di una funzione multivariata?}

Sia $f: A \subseteq \mathbb{R}^n \to \mathbb{R}$ una funzione definita su un aperto $A$ e sia $\mathbf{x}_0 \in A$ un punto di $A$.

\begin{definizione}[Massimo e minimo locale] Se $\exists B(\mathbf{x}_0, r)$ con $r>0$ (intorno di $\mathbf{x}_0$) per cui $\forall x \in A \cap B(\mathbf{x}_0, r)$ $f(\mathbf{x}_0) 
\geq f(\mathbf{x})$ allora $\mathbf{x}_0$ è un punto di minimo locale di $f$, mentre se $f(\mathbf{x}_0) \leq f(\mathbf{x})$ allora $\mathbf{x}_0$ è un punto di massimo locale di $f$.
\end{definizione}
Si dice massimo (minimo) forte se la disuguaglianza è stretta, per $\mathbf{x} \neq \mathbf{x}_0$.

I punti di massimo e minimo relativi si chiamano anche punti di estremo locale.
\newpage
\textbf{105. Qual è la differenza tra ottimizzazione {\color{red}vincolata e libera}?}

L'ottimizzazione libera si occupa di trovare i massimi e minimi di una funzione senza alcuna restrizione sul dominio. 
In questo caso, si cerca di identificare i punti in cui la funzione raggiunge i suoi valori estremi all'interno dell'intero spazio definito dalla funzione stessa.
Invece, l'ottimizzazione vincolata implica la ricerca di massimi e minimi di una funzione soggetta a determinate restrizioni o vincoli,
di solito espressi come equazioni o disequazioni.

\section{Ottimizzazione libera}
\textbf{106. Enunciare e dimostrare {\color{red}il teorema di Fermat} per funzioni multivariate.}
\begin{teorema}[Teorema di Fermat] Sia $f$ definita su un dominio $D = A \cup \partial A$ con $A$ aperto, allora $f: D \subset \mathbb{R}^n \to \mathbb{R}$ e 
sia $\mathbf{x}_0 \in D$  o sia $f: A \subseteq \mathbb{R}^n \to \mathbb{R}$ e $\mathbf{x}_0 \in A$ un punto di massimo (minimo) locale per $f$.

Se $f$ è differenziabile in tale punto $\mathbf{x}_0$, il gradiente di $f$ in $\mathbf{x}_0$ è nullo.
\end{teorema}
Questa è una condizione necessaria ma non sufficiente per l'esistenza di massimi e minimi locali perché esistono punti, detti critici o stazionari, che hanno gradiente nullo ma non sono né massimi né minimi (es. punti di sella o colle).

\textbf{\color{thmcolor}DIMOSTRAZIONE}

Sia $\mathbf{x}_0 \in A$ un punto di estremo relativo per $f$, ad esempio un massimo locale, allora $\mathbf{x}_0$ è un punto di estremo relativo per f lungo una qualsiasi retta passante per il punto.
Consideriamo una direzione arbitraria data da un vettore unitario $\mathbf{v} \in \mathbb{R}^n$ e definiamo la funzione ausiliaria $F(t) = f(\mathbf{x}_0 + t\mathbf{v})$ in un intorno di $t=0$,
ovvero con $\delta > 0$ tale che $B(\mathbf{x}_0, \delta) \subset A$.

Tale funzione reale di una variabile $F(t)$ con $t \in (-\delta, \delta)$  ha in $t=0$ un punto di massimo locale per cui, per il teorema di Fermat, si ha:
\[
F'(0) = \langle \nabla f(\mathbf{x}_0), \mathbf{v} \rangle = 0
\]
\hfill{\color{thmcolor}$\square$}

\subsection{Test dell'hessiana}
\textbf{107. Come si studia {\color{red}la natura dei punti critici}?}

Per le funzioni ad una variabile dopo aver supposto che un punto fosse critico e che la $f \in C^2$ si studiava il segno della derivata seconda.
Nel caso di funzioni di più variabili lo studio viene fatto  con la matrice Hessiana.

Sia $f: A \subseteq \mathbb{R}^2 \to \mathbb{R}$ con $A$ aperto e $f \in C^2(A)$, ossia esistono le 4 derivate seconde e sono continue.

Sia $P_0 = (x_0, y_0)$ un punto critico per $f$, ovvero $\nabla f(P_0) = \mathbf{0}$ e supponiamo che il determinante della matrice Hessiana nel punto non sia nullo: $\det Hf(x_0, y_0) \neq 0$.
\[
\det Hf(x_0, y_0) = \det 
\begin{bmatrix}
f_{xx} & f_{xy} \\
f_{yx} & f_{yy}
\end{bmatrix}
= f_{xx}f_{yy} - (f_{xy})^2
\]
(Ricordando che per il Teorema di Schwarz $f_{xy} = f_{yx}$, quindi la matrice è simmetrica).

La classificazione avviene come segue:
\begin{itemize}
    \item Se $\det H > 0$ e $f_{xx} > 0 \implies$ minimo locale;
    \item Se $\det H > 0$ e $f_{xx} < 0 \implies$ massimo locale;
    \item Se $\det H < 0 \implies$ punto di sella.
\end{itemize}
Se $\det H = 0$, il test è inefficace e bisogna procedere con altri metodi.

\subsection{Forme Quadratiche}
\textbf{108. Come possiamo {\color{red}dimostrare} quanto detto sul {\color{red}test dell'hessiana}?}

Quando sei in un punto critico, la parte lineare (il piano tangente) è piatta (zero). Quindi, per capire se si sale o scende, bisogna guardare il termine successivo, quello di secondo grado della formula di Taylor.

Quindi, per capire l'origine del test precedente, analizziamo la formula di Taylor al secondo ordine. 
Il segno della variazione locale della funzione dipende dal termine di secondo grado:
\[
f(\mathbf{x}_0 + \mathbf{h}) - f(\mathbf{x}_0) \approx \frac{1}{2} \underbrace{\langle Hf(\mathbf{x}_0)\mathbf{h}, \mathbf{h} \rangle}_{q(\mathbf{h})}
\]
Il termine $q(\mathbf{h})$ è una forma quadratica.

\vspace{0.3em}
\textbf{109. Cos'è {\color{red} una forma quadratica}?}
\begin{definizione}[Forma Quadratica] Una forma quadratica in $\mathbf{h} \in \mathbb{R}^n$ è un polinomio omogeneo di secondo grado nelle variabili $h_1, \dots, h_n$.
Ad ogni forma quadratica $q(\mathbf{h})$ è associata una matrice simmetrica $A$ tale che:
\[
q(\mathbf{h}) = \langle A\mathbf{h}, \mathbf{h} \rangle = \sum_{i,j=1}^n a_{ij} h_i h_j
\]
Data la simmetria ($a_{ij} = a_{ji}$), i coefficienti del polinomio corrispondono agli elementi della matrice (i termini misti si dividono per 2).
\end{definizione}

Dato il polinomio omogeneo:
\[ q(\mathbf{h}) = 3h_1^2 + 10h_1h_2 + 27h_2^2 \]
La matrice simmetrica associata $A$ è:
\[
A = 
\begin{bmatrix}
3 & 5 \\
5 & 27
\end{bmatrix}
\]
Notare come il coefficiente $10$ di $h_1h_2$ sia stato diviso equamente tra $a_{12}$ e $a_{21}$.

\vspace{0.3em}
\textbf{110. Studiando la natura dei punti critci, come si classifica una {\color{red}forma quadratica} in base alla {\color{red}matrice associata}?}

Per determinare la natura del punto critico, dobbiamo studiare il segno di $q(\mathbf{h})$ per $\mathbf{h} \neq \mathbf{0}$.

Sia $A$ la matrice Hessiana associata. Essendo simmetrica reale, possiede $n$ autovalori reali $\lambda_1, \dots, \lambda_n$.

\begin{itemize}
    \item $A$ è definita positiva: $q(\mathbf{h}) > 0, \forall \mathbf{h} \neq \mathbf{0}$.
    \begin{itemize}
        \item Condizione sugli autovalori: $\lambda_i > 0$ per tutti gli $i$;
        \item Conseguenza: Punto di minimo.
    \end{itemize}
    
    \item $A$ è definita negativa: $q(\mathbf{h}) < 0, \forall \mathbf{h} \neq \mathbf{0}$.
    \begin{itemize}
        \item Condizione sugli autovalori: $\lambda_i < 0$ per tutti gli $i$;
        \item Conseguenza: Punto di massimo.
    \end{itemize}
    
    \item $A$ è indefinita: Esistono $\mathbf{h}_1, \mathbf{h}_2$ tali che $q(\mathbf{h}_1) > 0$ e $q(\mathbf{h}_2) < 0$.
    \begin{itemize}
        \item Condizione sugli autovalori: Esistono autovalori discordi (almeno un $\lambda > 0$ e un $\lambda < 0$).
        \item Conseguenza: Punto di sella.
    \end{itemize}
\end{itemize}

\textbf{111. Come si collegano {\color{red}determinante e traccia} della matrice agli {\color{red}autovalori} per il {\color{red}test dell'hessiana}?}

Nel caso specifico di 2 variabili, si può dire che gli autovalori $\lambda_1, \lambda_2$ sono le radici del polinomio caratteristico
\[
 \det(A - \lambda I) = 0 \implies \lambda^2 - \text{tr}(A)\lambda + \det(A) = 0
\]
E valgono le relazioni fondamentali:
\[ \det(A) = \lambda_1 \cdot \lambda_2 \quad ; \quad \text{tr}(A) = \lambda_1 + \lambda_2 \]

Quindi la matrice simemtrica $A$ di dimensione $2 \times 2$ associata alla forma quadratica $q(h,k)$ è:
\[ A = 
\begin{bmatrix} a_{11}-\lambda & a_{12} \\
 a_{12} & a_{22} - \lambda
\end{bmatrix}
 = \lambda^2 - \underbrace{(a_{11} + a_{22})}_{\text{tr}(A)}\lambda + \underbrace{a_{11} \cdot a_{22}-a_{12}^2}_{\det(A)}=0
\] 

Questo ci permette di spiegare il test dell'hessiana in modo più intuitivo:
\begin{enumerate}
    \item Se $\det A > 0$, ovvero il prodotto $\lambda_1 \lambda_2$ è positivo $\implies$ autovalori concordi.
    \begin{itemize}
        \item Se $a_{11} > 0$ (o traccia $>0$), allora sono entrambi positivi $\to$ minimo.
        \item Se $a_{11} < 0$ (o traccia $<0$), allora sono entrambi negativi $\to$ massimo.
    \end{itemize}
    \item Se $\det A < 0$, ovvero il prodotto $\lambda_1 \lambda_2$ è negativo $\implies$ autovalori discordi $\to$ sella.
\end{enumerate}

\textbf{112. Perché si usano {\color{red}le forme quadratiche per spiegare il test dell'hessiana}?}

Possiamo riassumere il ragionamento in tre passaggi chiave che trasformano un problema di geometria in uno di algebra.

\begin{enumerate}
    \item \textbf{Il problema della "pianura" (Fermat non basta):} 
    Quando siamo in un punto critico ($\nabla f = 0$), il piano tangente è orizzontale. La parte lineare della funzione si annulla. Per capire se siamo su una cima, in una buca o su una sella, dobbiamo guardare il "termine successivo" dello sviluppo di Taylor: il termine di secondo grado (quadratico).
    
    \item \textbf{Gli autovalori sono le "curvature vere":} 
    La matrice Hessiana descrive la curvatura, ma spesso contiene "termini misti" ($f_{xy}$) che creano confusione perché dipendono dal sistema di coordinate scelto ($x, y$).
    Gli \textbf{autovalori} ($\lambda$) rappresentano le curvature lungo le \textbf{direzioni principali}, ovvero quelle direzioni speciali in cui la superficie curva senza torsioni.
    \begin{itemize}
        \item Un $\lambda$ positivo indica una curvatura verso l'alto (come una parabola $x^2$).
        \item Un $\lambda$ negativo indica una curvatura verso il basso (come $-x^2$).
    \end{itemize}
    Se gli autovalori sono tutti positivi, la funzione curva verso l'alto in ogni direzione (scodella $\to$ minimo). Se sono discordi, in una direzione sale e nell'altra scende (sella).

    \item \textbf{L'Equazione Caratteristica come "traduttore":}
    Per trovare gli autovalori usiamo l'equazione caratteristica:
    \[ \det(H - \lambda I) = 0 \]
    In due variabili, questa equazione ci svela una scorciatoia fondamentale:
    \[ \lambda_1 \cdot \lambda_2 = \det(H) \]
    Questo spiega perché il \textbf{Test dell'Hessiana} funziona guardando solo il determinante:
    \begin{itemize}
        \item Se $\det H < 0$, il prodotto $\lambda_1 \lambda_2$ è negativo. L'unico modo per ottenere un meno è moltiplicare un numero positivo e uno negativo. Ergo: autovalori discordi $\implies$ \textbf{Sella}.
        \item Se $\det H > 0$, il prodotto è positivo. Gli autovalori hanno lo stesso segno (concordi). Basta controllarne uno (tramite la traccia o $f_{xx}$) per capire se sono entrambi positivi (Minimo) o negativi (Massimo).
    \end{itemize}
\end{enumerate}

\subsection{Caso dubbio: il test dell'hessiana non funziona}
\textbf{113. Cosa si fa quando {\color{red}il test dell'hessiana fallisce} ($\det H = 0$)?}

Quando il determinante è nullo, la matrice Hessiana è "degenere" e non ci dà informazioni sufficienti sulla curvatura (la funzione è "troppo piatta" per l'approssimazione quadratica).

In questo caso, si deve tornare alla definizione di estremo locale, studiando direttamente il segno della variazione della funzione in un intorno del punto critico $P_0=(x_0, y_0)$.

Si definisce l'incremento:
\[ \Delta f(x,y) = f(x,y) - f(x_0, y_0) \]
Si studia il segno di $\Delta f$ in un intorno molto piccolo di $P_0$:

\begin{enumerate}
    \item \textbf{Minimo Locale:} Se esiste un intorno in cui $\Delta f(x,y) \ge 0$ per ogni punto (ovvero la funzione assume sempre valori maggiori o uguali a $f(P_0)$);
    
    \item \textbf{Massimo Locale:} Se esiste un intorno in cui $\Delta f(x,y) \le 0$ per ogni punto;
    
    \item \textbf{Punto di Sella:} Se in \textit{qualsiasi} intorno di $P_0$ esistono sia punti in cui $\Delta f > 0$ sia punti in cui $\Delta f < 0$. 
    Graficamente, studiando il segno sul piano $xy$, vedremo delle regioni positive e negative che convergono nel punto critico.
\end{enumerate}


\section{Ottimizzazione vincolata}
\textbf{114. Enunciare e dimostrare {\color{red} il Teorema di Weierstrass} per funzioni multivariate.}
Se si studia la natura dei punti critci quando l'insieme di definizione è compatto, si può usare il Teorema di Weierstrass.
\begin{teorema}[Teorema di Weierstrass] Sia $f: K \subseteq \mathbb{R}^n \to \mathbb{R}$ una funzione continua definita su un insieme compatto $K \subset \mathbb{R}^n$, ossia $K$ è un sottoinsieme chiuso e limitato di $\mathbb{R}^n$.
Allora $f$ assume sia il massimo che il minimo assoluto su in almeno due punti di $K$.
\end{teorema}
Si può anche definire il teorema come:
\[
\exists (x_{\text{min}}, y_{\text{min}}), (x_{\text{max}}, y_{\text{max}}) \in K : f(x_{\text{min}}, y_{\text{min}}) \leq f(x,y) \leq f(x_{\text{max}}, y_{\text{max}}) \quad \forall (x,y) \in K
\]

\textbf{115. In cosa consiste il problema dell'{\color{red}ottimizzazione vincolata}?}

A differenza dell'ottimizzazione libera, dove cerchiamo estremi su un aperto $A \subseteq \mathbb{R}^n$, qui cerchiamo i massimi e minimi di una funzione $f(x,y)$ limitatamente ai punti che appartengono a un insieme $V$ (detto \textit{vincolo}).
\[ V = \{ (x,y) \in \mathbb{R}^2 : \underbrace{g(x,y) = k}_{g(x,y)-k=0} \} \subseteq \mathbb{R}^2 \quad \text{con }g \in C^1 \]
In genere $V$ è una curva di livello (es. una circonferenza, una retta) definita implicitamente da un'equazione $g(x,y)=0$ (o $k$).

Quindi risulta possibile esplicitare una funzione rispetto ad un'altra (teorema della funzione implicita):
\[
y=y(x) \text{ allora il vincolo diventa } g(x,y(x)), \text{ una funzione ad una variabile} 
\]

\subsection{Parametrizzazione}
\textbf{116. Come funziona {\color{red}la parametrizzazione o riduzione a una variabile}?}

Se l'equazione del vincolo $g(x,y)=0$ è semplice, possiamo parametrizzare la curva ($x(t), y(t)$) trasformando il vincolo in un sostegno ad una curva $V=(\gamma[a,b])$.
\[
\begin{cases}
x = x(t) \\
y = y(t) \\
\end{cases} \quad t \in [0,h]
\]
Questo metodo è comodo per vincoli lineari (es. $3x+5y=7$) ma difficile per vincoli complessi.

\subsection{Moltiplicatori di Lagrange}
\textbf{117. Enunciare il {\color{red}Teorema dei Moltiplicatori di Lagrange}.}

Siano $f, g: A \to \mathbb{R}^2$ funzioni di classe $C^1(A)$, supponendo che $\mathbf{x}_0 \in V$ un punto di estremo locale per f sull'insieme $V = \{ (x,y)\in A : g(x,y) = k \}$.

Se $\mathbf{x}_0$ è un \textit{punto regolare} per il vincolo (ovvero $\nabla g(x,y) \neq (0,0)$), allora esiste un numero reale $\lambda \in \mathbb{R}$ (detto moltiplicatore di lagrange) tale che:
\[ \nabla f(x,y) = \lambda \nabla g(x,y) \]
Geometricamente, significa che nel punto di estremo le curve di livello di $f$ sono tangenti al vincolo $V$ (i gradienti sono paralleli).

\vspace{0.3em}
\textbf{118. Cos'è la {\color{red}Funzione Lagrangiana} e come si usa?}

Per trovare i candidati punti di estremo vincolato definiamo una funzione detta \textbf{Lagrangiana}:
\[ \mathcal{L}(x, y, \lambda) = f(x,y) - \lambda [g(x,y) - k] \]
che ha la stessa regolarità della funzione di cui è composta.

I punti critici vincolati si trovano cercando i punti stazionari di $\mathcal{L}$ (dove il gradiente "completo" si annulla). Si risolve il sistema:
\[
\nabla \mathcal{L}(x,y,\lambda) = \mathbf{0} \Leftrightarrow
\begin{cases}
\frac{\partial f}{\partial x} - \lambda g_x = 0 \\
\frac{\partial f}{\partial y} - \lambda g_y = 0 \\
-(g(x,y) - k) = 0 \quad (\text{che è il vincolo stesso})
\end{cases}
\]
Le soluzioni $(x, y, \lambda)$ ci forniscono i candidati $(x,y)$ da valutare, che vanno poi testati con altri metodi per trovare i massimi e minimi assoluti.

\vspace{0.3em}
\textbf{119. Esempio di {\color{red}ottimizzazione vincolata} con {\color{red}sostituzione} e {\color{red}Lagrange}.}

\textit{Trovare max e min di $f(x,y) = (x-1)^2 - y^2$ soggetta al vincolo $x^2+y^2=1$.}

Il vincolo è la circonferenza unitaria (compatto $\to$ Weierstrass garantisce l'esistenza).

\textbf{Metodo 1 (Sostituzione):}
Dal vincolo: $y^2 = 1 - x^2$ (con $x \in [-1, 1]$).
Sostituiamo in $f$:
\[ h(x) = (x-1)^2 - (1-x^2) = x^2 - 2x + 1 - 1 + x^2 = 2x^2 - 2x \]
Derivata: $h'(x) = 4x - 2 \implies x = 1/2$.
Confrontiamo i valori in $x=1/2$ e agli estremi $x=\pm 1$:
\begin{itemize}
    \item $x=1 \implies y=0 \implies f(1,0) = 0$
    \item $x=-1 \implies y=0 \implies f(-1,0) = 4$ (Max)
    \item $x=1/2 \implies y=\pm\sqrt{3}/2 \implies f(1/2, \pm\sqrt{3}/2) = -1/2$ (Min)
\end{itemize}

\textbf{Metodo 2 (Lagrange):}
Costruiamo $\mathcal{L}(x,y,\lambda) = (x-1)^2 - y^2 - \lambda(x^2+y^2-1)$.
Sistema:
\[
\begin{cases}
2(x-1) - 2\lambda x = 0 \\
-2y - 2\lambda y = 0 \implies -2y(1+\lambda) = 0 \\
x^2+y^2 = 1
\end{cases}
\]
Dalla seconda equazione: $y=0$ o $\lambda = -1$.
\begin{itemize}
    \item Se $y=0$: dal vincolo $x = \pm 1$. Ottengo i punti $(1,0)$ e $(-1,0)$.
    \item Se $\lambda = -1$: dalla prima equazione $2(x-1) = -2x \implies 2x-2 = -2x \implies 4x=2 \implies x=1/2$.
    Dal vincolo $y^2 = 1 - (1/4) = 3/4 \implies y = \pm \sqrt{3}/2$.
\end{itemize}
Si ritrovano gli stessi 4 punti candidati.

\section{Integrazione}

\textbf{119. Cosa si intende per {\color{red}integrale doppio}?}

\vspace{0.1em}
Mentre l'integrale dell'analisi unidimensionale $\int_{a}^{b} f(x)dx$ calcola l'area tra la curva e l'asse $x$, l'integrale doppio $\iint_{R} f(x,y) \, dx \, dy$ calla il volume compreso
tra la superficie definita da $z = f(x,y)$ e il piano $xy$ sopra un dominio $R$ nel piano.

Sia una funzione $f: R \to \mathbb{R}$ definita su un rettangolo $R = [a,b] \times [c,d]$, supponiamo che $D_1$ e $D_2$ siano due partizioni di $[a,b]$ e $[c,d]$ rispettivamente definite da $n+1$ e $m+1$ punti:
\[
D_1=\{a=x_0, x_1, \dots, x_n=b\}\\\ \quad
D_2=\{c=y_0, y_1, \dots, y_m=d\}
\]
Consideriamo $D = D_1 \times D_2$ un dominio nel piano $xy$, esso ci dà una suddivisione del rettangolo in $r_{ij}$, dove si definisce $r_{ij} = [x_{i-1}, x_i] \times [y_{j-1}, y_j]$.

Il rettangolo $R$ viene quindi suddiviso in $n \cdot m$ sotto-rettangoli $r_{ij}$.

Ora, supponiamo che $f$ sia limitata sul rettangolo $R$, tale che esistono due numeri reali $m$ e $M$ per cui $m \le f(x,y) \le M$ per ogni $(x,y) \in R$.
Se per ogni rettangolino considero $m_{i,j}=inf f(x,y)$ come l'estremo inferiore (il valore minimo) assunto dalla $f(x,y)$, il volume del parallalepipedo sotto la superficie è
\[
\sum_{i,j=1}^{n,m} m_{i,j} \cdot \underbrace{(x_i-x_{i-1}) \cdot (y_j-y_{j-1})}_{\text{Area}(R_{i,j})} 
\]
La somma di tutti questi volumi viene definita come la somma inferiore: $s=(f,D)$.

Stesso ragionamento vale per la somma superiore $S(f,D)$, dove si considera $M_{i,j}=sup f(x,y)$ come l'estremo superiore (il valore massimo) assunto dalla $f(x,y)$.
\[
\sum_{i,j=1}^{n,m} M_{i,j} \cdot \underbrace{(x_i-x_{i-1}) \cdot (y_j-y_{j-1})}_{\text{Area}(R_{i,j})} 
\]

Queste due somme sono legate dalla relazione proveniente dal teorema della media integrale
\[
m(b-a)(d-c) \le s \le S \le M(b-a)(d-c) \quad \forall \text{ suddivisione di } R
\]

Al variare di $D$, le somme inferiori e superiori variano, ma rimangono sempre limitate e ben definite tra $m(b-a)(d-c)$ e $M(b-a)(d-c)$.

Quindi, come nel caso unidemnsionale si dimostra che vale la relazione per cui $\text{sup}_D s (f,D) \le \text{inf}_D S(f,D)$, che nel caso di una uguaglianza ci permette di definire una funzione integrabile.

\begin{definizione}[Integrale Doppio] Una funzione $f: R \to \mathbb{R}$ limitata su $R$ dove $R = [a,b] \times [c,d]$. La funzione $f$ si dice integrabile secondo Riemann su $R$ se
\[
\text{sup}_D s (f,D) = \text{inf}_D S(f,D)
\] 
\end{definizione}
In tal caso, il valore comune(un numero reale) viene detto integrale doppio di $f$ su $R$ e si indica con
\[
\iint_{R} f, \quad I(f,R), \quad \iint_{R} f(x,y) \, dx \, dy
\]

\textbf{120. Qual è {\color{red}l'interpretazione geometrica} con cui si definisce l'integrale doppio?}
Se si ha ce la funzione $f(x,y) \ge 0$ per ogni $(x,y) \in R$, l'integrale doppio rappresenta il volume della regione solida compresa tra la superficie $z=f(x,y)$ e 
il piano $xy$ sopra il rettangolo $R$, ossia un cilindroide curvilineo.
\begin{figure}[H] 
    \centering
    \includegraphics[width=0.5\linewidth]{../Immagini/Cilindroide.png}
\end{figure}
Ogni addendo nelle somme inferiore e superiori è il volume di un parallelepiped di base il rettangolo $r_{ij}$ e altezza rispettivamente $m_{i,j}$ e $M_{i,j}$.

\textbf{121. Enunciare il {\color{red}teorema della continuità} per gli integrali doppi.}
\begin{teorema}[Teorema sulla continuità]Sia $R$ un rettangolo chiuso e limitato in $\mathbb{R}^2$ definito come $R=[a,b] \times [c,d]$ e sia $f: R \to \mathbb{R}$ una funzione a più variabili.
Se la funzione è continua su tutto il rettangolo $R$, allora $f$ è integrabile secondo Riemann su $R$.
\end{teorema}

\subsection{Calcolo Integrale sui rettangoli}
\textbf{122. Come si calcolano gli integrali doppi su {\color{red}rettangoli}? (Formule di riduzione)}

Sia $f: R \to \mathbb{R}$ una funzione continua definita su un rettangolo $R = [a,b] \times [c,d]$, se $f(x,y)=g(x) \cdot h(y)$ con $g: [a,b] \to \mathbb{R}$ e $h: [c,d] \to \mathbb{R}$ integrabili su i rispettivi intervalli,
allora $f$ è integrabilesu $R$ e vale la seguente proprietà di separazione:
\[ \iint_R f(x,y) \, dx \, dy = \left( \int_a^b g(x) \, dx \right) \cdot \left( \int_c^d h(y) \, dy \right) \]
Questo è il caso semplice in cui la funzione si può "separare" in due funzioni di una sola variabile.

\vspace{0.3em}
\textbf{123. Come si definsce {\color{red}il teorema di Fubini-Tonelli} per gli integrali doppi?}

Nel caso generale si definisce il teorema di Fubini-Tonelli, che permette di calcolare l'integrale doppio come integrali singoli successivi (integrali iterati).
Sia $f: R \to \mathbb{R}$ una funzione continua definita su un rettangolo $R = [a,b] \times [c,d]$, allora $f$ è integrabile su $R$ e vale la seguente proprietà di riduzione:
\[ \iint_R f(x,y) \, dx \, dy = \int_a^b \left[ \int_c^d f(x,y) \, dy \right] dx = \int_c^d \left[ \int_a^b f(x,y) \, dx \right] dy \]
Quindi, prima integro rispetto ad una variabile (tenendo l'altra fissa), e poi integro il risultato rispetto all'altra variabile.
Geometricamente significa "affettare" il volume lungo un asse e poi sommare le aree delle fette. L'ordine di integrazione è indifferente (per funzioni continue).

\subsection{Integrali doppi su domini generali}
\textbf{124. Se invece di rettangolsi si avessero dei {\color{red}domini più generali} per definire un integrale doppio?}

Sia $D$ una regione del piano limitata e sia $f$ definita e limitata su questo dominio.
Estendiamo la funzione $f$ a tutto il rettangolo $R$ che contiene $D$, ponendo $f(x,y)=0$ per ogni punto $(x,y) \notin D$.

\begin{definizione} Sia f una funzione definita e limitata su un dominio limitato $D \subset \mathbb{R}^2$.
La funzione $f$ si dice integrabile secondo Riemann su $D$ se la funzione estesa $\tilde{f}$ è integrabile secondo Riemann sul rettangolo $R$ che contiene $D$.
In tal caso, l'integrale doppio di $f$ su $D$ è definito come:
\[  \iint_D f(x,y) \, dx \, dy = \iint_R \tilde{f}(x,y) \, dx \, dy \]
\end{definizione}

\textbf{125. Perché non tutti gli insiemi vanno bene e bisogna che {\color{red}insieme sia misurabile}?}
Non tutti gli insiemi possono essere misurabili perché non siamo in grado di dire con certezza quanto è grande la sua area. E se non conosciamo l'area di base non possiamo calcolare il volume che si sta sopra.
Un sottoinsieme limitato $D \subset \mathbb{R}^2$ è misurabile secondo Peano-Jordan se la funzione $f$ (identicamente $1$ su $D$ e $0$ fuori) è integrabile secondo Riemann sul rettangolo che contiene $D$.
In tale caso 
\[
|D| = \iint_{D} 1 \, dx \, dy \implies \text{area di }D
\]
In generale, ogni rettangolo $R = [a,b] \times [c,d]$ è misurabile secondo Peano-Jordan, e la sua misura è data da:
\[ |R| = (b-a)(d-c) \]
che è l'area del rettangolo.

Tutti gli insiemi che conosciamo dalla geometria elementare (poligoni, cerchi, ecc.) sono misurabili secondo Peano-Jordan e la loro misura corrisponde alla formula delle loro aree.

\vspace{0.3em}
\textbf{126. C'è un esempio di un poligono che non è {\color{red}misurabile secondo Peano-Jordan}?}
Sia Q un quadrato $[1,0] \times [0,1]$ con coordinate razionali e sia una funzione f $: Q \to \mathbb{R}$ definita come:
\[ f(x,y) = \begin{cases} 1 & \text{se } x,y \in \mathbb{Q} \\ 0 & \text{altrimenti} \end{cases} \quad \text{Funzione di Dirichlet} \]
Questa funzione non è integrabile secondo Riemann su Q, perché in ogni rettangolino $r_{ij}$, per quanto piccolo, la funzione assume sia il valore 0 che il valore 1 (i numeri razionali sono densi nei reali).
Quindi non esiste la somma inferiore uguale alla somma superiore, e di conseguenza l'insieme non è misurabile secondo Peano-Jordan.

\vspace{0.3em}
\textbf{127. Quali sono le {\color{red}proprietà} principali degli integrali doppi?}

Analogamente agli integrali singoli:
\begin{itemize}
    \item \textbf{Linearità:} L'integrale di una combinazione lineare è la combinazione lineare degli integrali.
    \[ \iint_D (\alpha f + \beta g) = \alpha \iint_D f + \beta \iint_D g \]
    \item \textbf{Additività sul dominio:} Se $D$ viene diviso in due sottodomini $D_1$ e $D_2$ che non si sovrappongono (se non sul bordo), l'integrale su $D$ è la somma degli integrali.
    \item \textbf{Monotonia:} Se $f(x,y) \le g(x,y)$ su tutto $D$, allora $\iint_D f \le \iint_D g$.
    \item \textbf{Relazione della funzione con il suo valore assoluto:} Se $f$ è integrabile su $D$, allora $|f|$ è integrabile su $D$ e vale:
    \[ \left| \iint_D f(x,y) \, dx \, dy \right| \le \iint_D |f(x,y)| \, dx \, dy \]
    E se $D$ è misurabile secondo Peano-Jordan, allora $|D| = \iint_{D} 1 \, dx \, dy$ e posso così definire:
    \[ \left| \iint_D f(x,y) \, dx \, dy \right| \le \iint_D |f(x,y)| \, dx \, dy \le \underbrace{\sup_{(x,y) \in D} |f(x,y)|}_{\text{massimo valore assoluto di } f \text{ in } D} \cdot |D| \]

\end{itemize}

\subsection{Calcolo Integrale sulle regioni semplici}
\textbf{128. Cosa si intende esattamente per {\color{red}Regione Normale} (o Dominio Semplice)?}

Non tutti i domini di integrazione sono rettangoli. Spesso ci troviamo a dover integrare su figure dove gli estremi dipendono dalla posizione.
Si dicono domini normali (o semplici) quelle regioni del piano delimitate da due rette parallele e dai grafici di due funzioni continue.
Ne esistono di due tipi:

\begin{itemize}
    \item Dominio normale rispetto all'asse $x$ \textbf{(Y-semplice):}
    
    È un insieme $D \subset \mathbb{R}^2$ che si proietta sull'asse $x$ in un intervallo chiuso $[a,b]$. Le sue "pareti" superiore e inferiore sono date da due funzioni continue $f_1(x)$ e $f_2(x)$.
    \[ D = \{ (x,y) \in \mathbb{R}^2 : a \le x \le b, \quad f_1(x) \le y \le f_2(x) \} \]

    \begin{figure}[H] 
    \centering
    \includegraphics[width=0.5\linewidth]{../Immagini/Ysemplice.png}
    \end{figure}

    $D$ è l'unione di tutti i segmenti verticali compresi tra le curve $y=f_1(x)$ e $y=f_2(x)$ per ogni $x$ in $[a,b]$ e si può scrivere come:
    \[ D = \bigcup_{x \in [a,b]} \{ x \} \times [f_1(x), f_2(x)] \]
    
    \item Dominio normale rispetto all'asse $y$ \textbf{(X-semplice):}

    È un insieme $D$ che si proietta sull'asse $y$ in un intervallo $[c,d]$. Le sue pareti destra e sinistra sono funzioni della $y$, diciamo $h_1(y)$ e $h_2(y)$.
    \[ D = \{ (x,y) \in \mathbb{R}^2 : c \le y \le d, \quad h_1(y) \le x \le h_2(y) \} \]
    
    $D$ è l'unione di tutti i segmenti orizzontali compresi tra le curve $x=h_1(y)$ e $x=h_2(y)$ per ogni $y$ in $[c,d]$ e si può scrivere come:
    \[ D = \bigcup_{y \in [c,d]} [h_1(y), h_2(y)] \times \{ y \} \]

    \begin{figure}[H] 
    \centering
    \includegraphics[width=0.5\linewidth]{../Immagini/Xsemplice.png}
    \end{figure}
\end{itemize}

Le misure dei due insimemi, pensando all'interpretazione geometrica, sono l'area delle regioni che si ottengono:
\[D_{y-\text{semplice}} = \int_a^b [f_2(x) - f_1(x)] \, dx \]
\[D_{x-\text{semplice}} = \int_c^d [h_2(y) - h_1(y)] \, dy \]


\textbf{129. Quali sono le {\color{red}formule di riduzione} per questi domini?}
\begin{teorema}[Formule di riduzione per domini normali] Ogni funzione continua $f: D \to \mathbb{R}$ definita su un dominio normale $D$ è integrabile secondo Riemann su $D$ e vale:
\begin{itemize}
    \item Se $D$ è Y-semplice:
    \[ \iint_D f(x,y) \, dx \, dy = \int_a^b \left( \int_{f_1(x)}^{f_2(x)} f(x,y) \, dy \right) dx \]
    
    \item Se $D$ è X-semplice:
    \[ \iint_D f(x,y) \, dx \, dy = \int_c^d \left( \int_{h_1(y)}^{h_2(y)} f(x,y) \, dx \right) dy \]  
\end{itemize}
\end{teorema}

\textbf{130. In generale come si definisce il calcolo gli integrali doppi su {\color{red}più domini semplici}?}

Supponiamo di avere $D_1, D_2, \dots,D_k$ domini semplici sul piano e supponiamo che a due a due non hanno punti in comune tranne che sui bordi.
Se $f$ è una funzione continua sull'unione $D = \bigcup_{i=1}^{k} D_i$, allora $f$ è integrabile su $D$ e vale la seguente proprietà di additività:
\[ \iint_D f(x,y) \, dx \, dy = \sum_{i=1}^{k} \iint_{D_i} f(x,y) \, dx \, dy \]

D è una regione composta da più domini semplici, decomponibile.

\subsection{Cambiamento di variabili con integrali doppi}
\textbf{131. Perché e quando si effettua un {\color{red}cambiamento di variabili}?}

Nell'analisi unidimensionale il cambio di variabile è possibile se la funzione è di classe $C^1$ e biunivoca (invertibile):
\[ \int_a^b f(x) \, dx = \int_{\phi^{-1}(a)}^{\phi^{-1}(b)} f(\phi(t)) \cdot \phi'(t) \, dt \]

Nell'analisi multidimensionale, spesso quando abbiamo domaini che presentano delle simmetrie ci conviene cambiare le variabili trasformando il dominio $D$ (nel piano $xy$) in un dominio $D'$ (nel piano $\rho, \theta$) molto più semplice (ad esempio un rettangolo), 
tramite una trasformazione biunivoca.

\textbf{132. Come si definisce {\color{red}un cambiamento di variabili} di una funzione integrabile secondo riemann nell'analisi multivariata?}

Sia $f: \mathbb{R}^2 \to \mathbb{R}^2$ (un vettore in un vettore) tale che
\[
(\rho, \theta) \to f(\rho, \theta) = (f_1(\rho, \theta), f_2(\rho, \theta)) = (x(\rho, \theta), y(\rho, \theta))
\]
dove $x(\rho, \theta) = \rho \cos \theta$ e $y(\rho, \theta) = \rho \sin \theta$.

La trasformazione deve essere biunivoca e di classe $C^1$ (derivabile con continuità).
Questo ci permette di definire la matrice, detta Jacobiana, della $f(\rho, \theta) \to (\rho \cos \theta, \rho \sin \theta)$:

\[ J_{f(\rho, \theta)} = 
\begin{bmatrix}
\nabla f_1(\rho, \theta) \\
\nabla f_2(\rho, \theta)
\end{bmatrix}= 
\renewcommand{\arraystretch}{2}
\begin{bmatrix} 
\frac{\partial f_1}{\partial \rho}(\rho, \theta)& \frac{\partial f_1}{\partial \theta}(\rho, \theta) \\
\frac{\partial f_2}{\partial \rho}(\rho, \theta) & \frac{\partial f_2}{\partial \theta}(\rho, \theta) 
\end{bmatrix} =
\renewcommand{\arraystretch}{1}
\begin{bmatrix} 
\cos \theta & -\rho \sin \theta \\
\sin \theta & \rho \cos \theta
\end{bmatrix} = det J_{f(\rho, \theta)} = \rho \cos^2 \theta + \rho \sin^2 \theta = \rho 
\]
per la proprietà trigonometrica fondamentale $\cos^2 \theta + \sin^2 \theta = 1$.
Se il determinante è diverso da zero, la trasformazione è invertibile.
Con $\rho = 0 \implies x = 0, y = 0$ l'origine e $(0, \theta) \to (0,0) \forall \theta$ quindi la trasformazione non è biunivoca in $(0,0)$.
Possiamo anche affermare che le funzioni trigonometriche sono periodiche (2$\pi$), comportando una non biunivocità con $(\rho, \theta)$ e $(\rho, \theta + 2\pi k)$ per ogni intero $k$.    

Ma se restringiamo il dominio di $\theta$ a $[0, 2\pi)$ o $[-\pi, \pi)$ e $\rho \ge 0$, la trasformazione diventa biunivoca.

Consideriamo $f:A \subseteq \mathbb{R}^2 \to \mathbb{R}^2$ con $(\rho, \theta) \to (\rho \cos \theta, \rho \sin \theta)$ e prendiamo $A=(0, +\infty) \times [0, 2\pi)$ o $(0, +\infty) \times [-\pi, \pi)$.
Quindi, consideriamo $(\rho, \theta) \in A$
\vspace{0.3em}

\textbf{133. Enunciare {\color{red}il teorema del cambiamento di variabili per gli integrali doppi}.}
\begin{teorema}[Cambiamento di variabili per integrali doppi] Sia $S$ un sottoinsieme di A un aperto misurabile secondo Peano-Jordan del piano $(\rho, \theta)$ e sia $F:A \subseteq \mathbb{R}^2 \to \mathbb{R}^2$ una trasformazione di 
classe $C^1$ biunivoca tale che $F(\rho, \theta) = (x(\rho, \theta), y(\rho, \theta))$ con Jacobiana $J_F(\rho, \theta)$.

Allora per ogni funzione integrabile (limitata e continua) su $T$ si ha che:
\[
\iint_{T} f(x,y) \ dx \ dy = \iint_{S} f(\rho \cos \theta, \rho \sin \theta) \underbrace{\rho}_{|det J_F(\rho, \theta)|} \ d\rho \ d\theta  
\]
con $T = F(S)$ e $S = F^{-1}(T)$.
\end{teorema}

\begin{figure}[H] 
    \centering
    \includegraphics[width=0.5\linewidth]{../Immagini/CambiamentoVariabili.png}
\end{figure}


\end{document}